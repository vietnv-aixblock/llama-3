{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9de02150-a926-4dd2-bf82-61b8af37ac12",
      "metadata": {
        "id": "9de02150-a926-4dd2-bf82-61b8af37ac12"
      },
      "source": [
        "# ```generate()``` and 🤗accelerate\n",
        "This notebook is for people using ```model.generate()``` with multiple GPUs. I ❤️ 🤗 accelerate but some things were not obvious to me.\n",
        "\n",
        "I wish I had these examples earlier when starting out. In my opinion, the most important functions to know are `split_between_processes` and `gather_object`.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece7097c-a784-4e23-acd7-c7ff98fd509b",
      "metadata": {
        "id": "ece7097c-a784-4e23-acd7-c7ff98fd509b"
      },
      "source": [
        "## 0. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73P9Si1aBzBs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73P9Si1aBzBs",
        "outputId": "cc8ae792-c497-49c4-90da-1ead88e0e7b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.20.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets wandb accelerate transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0339dbf-d954-41ee-8982-56ca77c76f11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0339dbf-d954-41ee-8982-56ca77c76f11",
        "outputId": "9ccda9ff-827e-42fa-ae1b-b2355c5e30ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtonyshark365\u001b[0m (\u001b[33mtonyshark365-aixblock\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching training on one GPU.\n",
            "DistributedType.NO\n"
          ]
        }
      ],
      "source": [
        "from accelerate import notebook_launcher\n",
        "from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\n",
        "import wandb\n",
        "wandb.login('allow',\"69b9681e7dc41d211e8c93a3ba9a6fb8d781404a\")\n",
        "def test():\n",
        "    from accelerate import Accelerator\n",
        "    accelerator = Accelerator()\n",
        "    print(accelerator.distributed_type)\n",
        "\n",
        "notebook_launcher(test, num_processes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c7f5354-d33d-4943-b272-7eb44a852975",
      "metadata": {
        "id": "5c7f5354-d33d-4943-b272-7eb44a852975"
      },
      "source": [
        "## 1. Hello world\n",
        "simplest example: create strings on each GPU and collect them using ```gather_object()```\n",
        "\n",
        "change ```num_processes``` to the number of GPUs in your system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e912559d-7ebb-4e92-9b3e-c053d7c6d834",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e912559d-7ebb-4e92-9b3e-c053d7c6d834",
        "outputId": "2ae85778-ba83-4194-b46e-c8b39bdd5903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching training on one GPU.\n",
            "['Hello this is GPU 0']\n"
          ]
        }
      ],
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "def hello_world():\n",
        "    from accelerate import Accelerator\n",
        "    from accelerate.utils import gather_object\n",
        "\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "    message= [f\"Hello this is GPU {accelerator.process_index}\"]\n",
        "    messages=gather_object(message)\n",
        "\n",
        "    accelerator.print(messages)\n",
        "\n",
        "notebook_launcher(hello_world, num_processes=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff0dce0-b22f-4e56-b598-b2dea9d2e657",
      "metadata": {
        "id": "3ff0dce0-b22f-4e56-b598-b2dea9d2e657"
      },
      "source": [
        "## 2. Multi-GPU text-generation\n",
        "* load a model on each GPU\n",
        "* distribute the prompts with ```split_between_processes```\n",
        "* have each GPU ```generate```\n",
        "* gather and output the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c347842a-2e54-4bd0-832b-e74d7493f760",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c347842a-2e54-4bd0-832b-e74d7493f760",
        "outputId": "ac6d176b-362a-4117-8651-240c07a8d86b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching training on one GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|bos|> The King is dead. Long live the Queen.\n",
            "\n",
            "The King of England is dead.\n",
            "\n",
            "The King of England is dead.\n",
            "\n",
            "The King of England is dead.\n",
            "\n",
            "The King of England is dead.\n",
            "\n",
            "The King of England is dead.\n",
            "\n",
            "The King of\n",
            "<|bos|> Once there were four children whose names were Peter, Susan, Edmund, and Lucy. This story is about a boy who was born in 1911.\n",
            "\n",
            "The story of Peter and Edmund is a story of a boy who was born in 1911.\n",
            "\n",
            "The story of David and Edmund is a\n",
            "<|bos|> The story so far: in the beginning, the universe was created. This has made a lot of people very angry.\n",
            "\n",
            "The story is that the universe is a very complicated thing. The universe is a very complicated thing. The universe is a very complicated thing. The universe is a very complicated thing. The universe is a very complicated thing. The universe is\n",
            "<|bos|> It was a queer, sultry summer, the summer they electrocuted the Rosenbergs, and I didn’t know what to expect.\n",
            "\n",
            "I was in the middle of a long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long,\n",
            "<|bos|> We were somewhere around Barstow on the edge of the desert when the drugs began to take hold. The police were on the scene, and we were able to get a few shots of the drug.\n",
            "\n",
            "We were in the middle of a drug bust, and we were in the middle of a drug bust. We were in the middle of a\n",
            "<|bos|> It was a bright cold day in April, and the clocks were striking thirteen. The sun was setting, and the sun was setting. The sun was setting, and the sun was setting.\n",
            "\n",
            "The sun was setting, and the sun was setting.\n",
            "\n",
            "The sun was setting, and the sun was setting.\n",
            "\n",
            "\n",
            "<|bos|> It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\n",
            "\n",
            "But the truth is that the man is not a man.\n",
            "\n",
            "The man is a man.\n",
            "\n",
            "The man is a man.\n",
            "\n",
            "The man is a man.\n",
            "\n",
            "The man is a man.\n",
            "\n",
            "The man\n",
            "<|bos|> The snow in the mountains was melting and Bunny had been dead for several weeks before we came to understand the gravity of the situation.\n",
            "\n",
            "We had been in the snow for a few days, and we had been in the snow for a while. We had been in the snow for a while, and we had been in the snow for a while. We had\n",
            "<|bos|> The sweat wis lashing oafay Sick Boy; he wis trembling.\n",
            "\n",
            "The man who was the first to be arrested for the murder of a 16-year-old girl in 1999 has been sentenced to life in prison.\n",
            "\n",
            "James S. Sweeney, \n",
            "<|bos|> 124 was spiteful. Full of Baby's venom.\n",
            "\n",
            "124 was full of baby's venom.\n",
            "\n",
            "124 was full of venom.\n",
            "\n",
            "124 was full of venom.\n",
            "\n",
            "124 was full of venom.\n",
            "\n",
            "1\n",
            "<|bos|> As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect.\n",
            "\n",
            "The 22-year-old had been living in a house in the village of Kaduna, in the state of Borno, for the past 10 years.\n",
            "\n",
            "He had been living in a house in the\n",
            "<|bos|> When Mary Lennox was sent to Misselthwaite Manor to live with her uncle everybody said she was a bit of a misogynist.\n",
            "\n",
            "But she was a bit of a misogynist.\n",
            "\n",
            "She was a bit of a misogynist.\n",
            "\n",
            "She was a bit of a misogynist.\n",
            "\n",
            "<|bos|> I write this sitting in the kitchen sink. I have a few things to do, but I have a few things to do. I have a few things to do, but I have a few things to do. I have a few things to do, but I have a few things to do\n"
          ]
        }
      ],
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "def hello_world():\n",
        "    from accelerate import Accelerator\n",
        "    from accelerate.utils import gather_object\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    import torch\n",
        "\n",
        "    # https://www.penguin.co.uk/articles/2022/04/best-first-lines-in-books\n",
        "    prompts_all=[\n",
        "        \"The King is dead. Long live the Queen.\",\n",
        "        \"Once there were four children whose names were Peter, Susan, Edmund, and Lucy. This story\",\n",
        "        \"The story so far: in the beginning, the universe was created. This has made a lot of people very angry\",\n",
        "        \"It was a queer, sultry summer, the summer they electrocuted the Rosenbergs, and I didn’t know what\",\n",
        "        \"We were somewhere around Barstow on the edge of the desert when the drugs began to take hold.\",\n",
        "        \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
        "        \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\",\n",
        "        \"The snow in the mountains was melting and Bunny had been dead for several weeks before we came to understand the gravity of\",\n",
        "        \"The sweat wis lashing oafay Sick Boy; he wis trembling.\",\n",
        "        \"124 was spiteful. Full of Baby's venom.\",\n",
        "        \"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect.\",\n",
        "        \"When Mary Lennox was sent to Misselthwaite Manor to live with her uncle everybody said she was\",\n",
        "        \"I write this sitting in the kitchen sink.\",\n",
        "    ]\n",
        "\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "    model_path=\"Locutusque/TinyMistral-248M\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        device_map={\"\": accelerator.process_index},\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    with accelerator.split_between_processes(prompts_all) as prompts:\n",
        "        outputs=[]\n",
        "        for prompt in prompts:\n",
        "            prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "            output_tokenized = model.generate(**prompt_tokenized, max_new_tokens=50)\n",
        "            outputs.append(tokenizer.decode(output_tokenized[0]))\n",
        "\n",
        "    outputs_gathered=gather_object(outputs)\n",
        "\n",
        "    for output in outputs_gathered:\n",
        "        accelerator.print(output)\n",
        "\n",
        "    with open('outputs.txt','w') as file:\n",
        "        file.write('\\n\\n'.join(outputs_gathered))\n",
        "\n",
        "notebook_launcher(hello_world, num_processes=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26fa3785-6a2d-4fa0-b214-9880936cd84b",
      "metadata": {
        "id": "26fa3785-6a2d-4fa0-b214-9880936cd84b"
      },
      "source": [
        "## 3. Generate while training Multi-GPUs\n",
        "This one adds a bit more, here we train a tiny LLM and see how it's output changes during training. These are the steps:\n",
        "* load ```Locutusque/TinyMistral-248M```\n",
        "* load ```timdettmers/openassistant-guanaco```\n",
        "* make up a few random ```prompts```\n",
        "* add a ```TrainerCallback``` to evaluate after each epoch\n",
        "    * distribute ```prompts``` among the GPUs\n",
        "    * on each GPU: split the received prompts into batches (```bs=8``` in the code below)\n",
        "    * batched inference with ```generate()```\n",
        "    * collect outputs using ```gather_object```\n",
        "    * log, print, whatever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfBFnY-LGU52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "cfBFnY-LGU52",
        "outputId": "d2cabd05-7a5d-4a61-f724-e94268f1fb82"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "str expected, not int",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5e714248f7c6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WORLD_SIZE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NCCL_DEBUG\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"INFO\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_LAUNCH_BLOCKING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_process_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nccl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(value)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: str expected, not int"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"Locutusque/TinyMistral-248M\"\n",
        "\n",
        "# Initialize distributed\n",
        "os.environ[\"RANK\"]=\"0\"\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '12345'\n",
        "os.environ['WORLD_SIZE'] = '1'\n",
        "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "torch.distributed.init_process_group(backend='nccl', rank=0, world_size=1)\n",
        "\n",
        "\n",
        "# Retrieve tensor parallel model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    tp_plan=\"auto\",\n",
        ")\n",
        "\n",
        "# Prepare input tokens\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "prompt = \"Can I help\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# Distributed run\n",
        "outputs = model(inputs)\n",
        "\n",
        "# https://huggingface.co/docs/transformers/en/performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc56dcd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = \"12355\"\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"1,2\"\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "\n",
        "model_name=\"//sentence-transformers/Llama-2-7b-hf\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "# dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "dataset_name = pd.read_parquet('/notebooks/output_data/data.parquet\")\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"llama-2-7b-miniguanaco\"\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, use_cache=False, device_map={\"\": 0}\n",
        ")\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "###########################################################################\n",
        "# LoRA config based on QLoRA paper\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        r=64,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "\n",
        "# prepare model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"llama-7-int4-dolly\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=6 if use_flash_attention else 4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,\n",
        "    tf32=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    disable_tqdm=True # disable tqdm since with packing values are in correct\n",
        ")\n",
        "\n",
        "\n",
        "from trl import SFTTrainer\n",
        "\n",
        "max_seq_length = 1056 # max sequence length for model and packing of the dataset\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field=\"text\",\n",
        "    packing=True,\n",
        "    # formatting_func=format_instruction,\n",
        "    args=args,\n",
        ")\n",
        "\n",
        "output_dir = \"~/Llama-2-7b-hf_results/v2/\"\n",
        "\n",
        "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
        "# save model\n",
        "trainer.save_model(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61d78d42-bde8-458f-a4d7-7643106d2eb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859,
          "referenced_widgets": [
            "be997add2bd74bb988b14562c6fec61f",
            "16eb846b69e847a9875f3840f3352ccc",
            "c56d4d4a85414516b8264070461bc195",
            "bd8735715a054c83a69dea6e42fbd6dd",
            "974fbb0e7d544f44a4beeb1bf4f69c85",
            "cfe25ca9704a409d81cd731d44aa7d4b",
            "5e47ac4a84634765b0bb8fd8d257e412",
            "0c73bdc9f1c2421a864120a7f30ebae6",
            "a45ae5465342451dacde365096172a32",
            "4b5901dd21884462bee8e251c6330c94",
            "5f29f854e9074a4ba923336d86de356d",
            "f7a9e4aac7ea4a299556171d9ea1f169",
            "50fb7dacbfe74b5eb7071b14e5bb42ad",
            "4ad2828781ea42499d337fc80abd0a39",
            "df1fbb0e29154c0a8e039265cf6a081a",
            "f22981a360354ad39cb145fb35e0fbb8",
            "3611ec7a1a7c425581f06673976be0ef",
            "0aae5612884546aaa78ed8a37b3ef6a9",
            "1bd1f67ad14a474fb81083a3ebc2951f",
            "da90355165704ecab629de2581763aad",
            "8f5c1995063f4c89becece4e13e6fbfd",
            "5e8619d5ed8a48af943a94fea5fb3722",
            "dfeb9cbd42d0416e81905f266047865a",
            "e4c0747fe832434a95e6e55d385bd8f7",
            "86103c47134c4f4a81c347e8554379b0",
            "7554b55cbffa4487b87e7e6f019783d9",
            "489efbd6329b4150ae12e5644322ec5d",
            "e7501f2a3d3c4cfa9deecdd9630cb09b",
            "2e3b7557c586464c85133495aeec153c",
            "205bf599c4c641d0913dd75d8d7541c7",
            "425e183e7e8e40a8b16c959e458031bf",
            "5c0f9fc62597497fb319dae7f39d7273",
            "60f4b758cfa04e4a9895c06118e16f2d",
            "8f1eae3f8f1740f28798f69f5e408838",
            "cf031fe019534a13b4ea9aada4782661",
            "db1ceec0ead94d2b9bb5b727c98336fa",
            "417c46bf99e84c12bc6959581b8ca508",
            "5ebac1d5989f4e8daed196723f0be7c0",
            "74469960e4624e0faa09b8096e587e42",
            "4192ea964dc24c9498f138d4c9af61e6",
            "8a9f401200e947e98f73f37685c82215",
            "a2c0faf25820445ba8186b06566c5fa9",
            "02b927db24e5499e9b6d624ab9a1b006",
            "29d2e0cd9caf4dad90cede23f5436848",
            "b135a5449b25433a8b91a198b2ded4a4",
            "6f98a5531d8f412dbd55340f9d5aa6cc",
            "103e368a7abc4029a64511e9ec3c41e9",
            "e30f25f1991846bbb9db91dc60903f9c",
            "6b4dd46e060146c4b30a4b6582ad67d3",
            "d4418b69d61749fa8bf48d5dfc8abc79",
            "7b2b65ef90e24672b0f6b1b4215c6699",
            "063d4e56c5934065a6f8f0e70646cc67",
            "2d71e06570c245bda6b820a3db8ca110",
            "464be5fb49db4bb39fb4c910ea2bcdcc",
            "56493cf9b2384e8e88d4e12279b62d3d",
            "b5bae5dc908f447fb01da787894c6fcd",
            "cf8add6bdaf046728defa40530e65e7d",
            "017a7d602ad74796b8b1994081464c2f",
            "ada6b90e1bd44d3ca647ca0da1c37caf",
            "a079494321d2453e8886ba032f47591d",
            "7022e1fe0b8b41deb432dbc80029ce4d",
            "8359f8a9b13142a297ad615703f19c42",
            "d77dd181c3ba42fcba2205e00d6635d8",
            "754f9d8259c14c7ba9313c05d9e1dd7a",
            "e744323c0636464f980b6ea1cc25033f",
            "4fdd821c9c9240418365250b8240d12d",
            "ecf18e4b27a54504a7fd718eae009869",
            "e6f6e7985ba345b7a839fcc3a4dad682",
            "b566bef94c5e48468e58d1b42fa5e3b8",
            "3c2655fc08e54138915588de0af525d3",
            "26a3a6a9a2d44870bad378aa6d223c15",
            "f42885b072a64782ba49ab75b27eb30c",
            "ced717d2ff5a46aea97f413fc04b3f66",
            "0c6f1fddbf3f4fa5aa6747f14bcc5ca2",
            "12d1846dd4bd4bc083bd453a7ae2636e",
            "438353e0676c4b498b5e60520634bd38",
            "f3b75fda034c43669e4f5d103cf88810"
          ]
        },
        "id": "61d78d42-bde8-458f-a4d7-7643106d2eb1",
        "outputId": "ba2d4f8a-cf61-4e18-f307-08c120c1359c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching training on one GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be997add2bd74bb988b14562c6fec61f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/395 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7a9e4aac7ea4a299556171d9ea1f169",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "openassistant_best_replies_train.jsonl:   0%|          | 0.00/20.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfeb9cbd42d0416e81905f266047865a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "openassistant_best_replies_eval.jsonl:   0%|          | 0.00/1.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f1eae3f8f1740f28798f69f5e408838",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/9846 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b135a5449b25433a8b91a198b2ded4a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/518 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5bae5dc908f447fb01da787894c6fcd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/9846 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecf18e4b27a54504a7fd718eae009869",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/518 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-23-d550708ece59>:125: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 28.12 MiB is free. Process 13809 has 14.71 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 493.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-d550708ece59>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m \u001b[0mnotebook_launcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhello_world\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/launchers.py\u001b[0m in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Launching training on one CPU.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_processes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d550708ece59>\u001b[0m in \u001b[0;36mhello_world\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m         ))\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0mnotebook_launcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhello_world\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2522\u001b[0m                     )\n\u001b[1;32m   2523\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3653\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3654\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3706\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3707\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3708\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3709\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3710\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1166\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    893\u001b[0m                 )\n\u001b[1;32m    894\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    896\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    624\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcausal_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mq_len\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m    545\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 28.12 MiB is free. Process 13809 has 14.71 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 493.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "def hello_world():\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, TrainingArguments, Trainer, TrainerControl, TrainerCallback, TrainerState\n",
        "    from accelerate import Accelerator\n",
        "    from accelerate.utils import gather_object\n",
        "    from datasets import load_dataset\n",
        "    import torch\n",
        "    import os\n",
        "    from accelerate.utils import AutocastKwargs\n",
        "\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model_path=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "    model.config.use_cache = False\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    # Load and tokenize dataset\n",
        "    dataset = load_dataset(\"timdettmers/openassistant-guanaco\")\n",
        "\n",
        "    def tokenize(element):\n",
        "        return tokenizer(element[\"text\"], truncation=True, max_length=512, add_special_tokens=False)\n",
        "    dataset_tokenized = dataset.map(tokenize, batched=True, num_proc=os.cpu_count(), remove_columns=[\"text\"])\n",
        "\n",
        "    # collate function - to transform list of dictionaries [ {input_ids: [123, ..]}, {.. ] to single batch dictionary { input_ids: [..], labels: [..], attention_mask: [..] }\n",
        "    def collate(elements):\n",
        "        tokenlist=[e[\"input_ids\"] for e in elements]\n",
        "        tokens_maxlen=max([len(t) for t in tokenlist])\n",
        "\n",
        "        input_ids,labels,attention_masks = [],[],[]\n",
        "        for tokens in tokenlist:\n",
        "            pad_len=tokens_maxlen-len(tokens)\n",
        "\n",
        "            # pad input_ids with pad_token, labels with ignore_index (-100) and set attention_mask 1 where content otherwise 0\n",
        "            input_ids.append( tokens + [tokenizer.pad_token_id]*pad_len )\n",
        "            labels.append( tokens + [-100]*pad_len )\n",
        "            attention_masks.append( [1]*len(tokens) + [0]*pad_len )\n",
        "\n",
        "        batch={\n",
        "            \"input_ids\": torch.tensor(input_ids),\n",
        "            \"labels\": torch.tensor(labels),\n",
        "            \"attention_mask\": torch.tensor(attention_masks)\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "    # List of prompts for evaluation\n",
        "    prompt_template=\"### Human: {}\\n### Assistant:\"\n",
        "    questions = [\n",
        "        \"Hello! Who are you? Introduce yourself please\",\n",
        "        \"How much is 2+2? Think step by step\",\n",
        "        \"What is on your mind?\",\n",
        "        \"Define artificial general intelligence\",\n",
        "        ] * 10 # expand. not creative enough for more\n",
        "    prompts = [ prompt_template.format(q) for q in questions ]\n",
        "\n",
        "    # Callback class for generation during training\n",
        "    class GenerateEvalCallback(TrainerCallback):\n",
        "        def __init__(self, prompts, accelerator):\n",
        "            self.prompts_all=prompts\n",
        "            self.accelerator=Accelerator()\n",
        "\n",
        "        # left pad for inference and tokenize\n",
        "        def prepare_prompts(self, prompts, tokenizer):\n",
        "            tokenizer.padding_side=\"left\"\n",
        "            prompts_tok=tokenizer(\n",
        "                prompts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding='longest',\n",
        "                truncation=False,\n",
        "                pad_to_multiple_of=8,\n",
        "                add_special_tokens=False).to(\"cuda\")\n",
        "            tokenizer.padding_side=\"right\"\n",
        "\n",
        "            return prompts_tok\n",
        "\n",
        "        def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, model, tokenizer, eval_dataloader, **kwargs):\n",
        "            model.eval()\n",
        "            model.config.use_cache = True\n",
        "\n",
        "            # split questions among GPUs\n",
        "            with accelerator.split_between_processes(self.prompts_all) as prompts:\n",
        "                # batched inference on each GPU\n",
        "                bs=8\n",
        "                batches=[prompts[i:i + bs] for i in range(0, len(prompts), bs)]\n",
        "                outputs=[]   # outputs per GPU\n",
        "                for prompt_batch in batches:\n",
        "                    prompts_tok=self.prepare_prompts(prompt_batch, tokenizer)\n",
        "                    with torch.no_grad():\n",
        "                        outputs_tok=model.generate(**prompts_tok, max_new_tokens=30).to(\"cpu\")\n",
        "                    outputs.extend([\n",
        "                        tokenizer.decode(outputs_tok[i][outputs_tok[i]!=tokenizer.pad_token_id])\n",
        "                        for i,t in enumerate(outputs_tok)\n",
        "                        ])\n",
        "            outputs_gathered=gather_object(outputs)  # collect results from all GPUs\n",
        "\n",
        "            # print a few to console\n",
        "            accelerator.print(f\"EPOCH {state.epoch:0.2f}:\")\n",
        "            for output in outputs_gathered[:5]:\n",
        "                accelerator.print(output)\n",
        "\n",
        "            # write all to file\n",
        "            if accelerator.is_main_process:\n",
        "                with open(f\"outputs_epoch-{state.epoch:0.2f}.txt\",'w') as file:\n",
        "                    file.write('\\n\\n'.join(outputs_gathered))\n",
        "\n",
        "            model.config.use_cache = False\n",
        "            return control\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"out\",\n",
        "        per_device_train_batch_size=16,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        logging_steps=1,\n",
        "        num_train_epochs=4,\n",
        "        learning_rate=0.001,\n",
        "        ddp_find_unused_parameters=False,\n",
        "        report_to=\"tensorboard\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        args=args,\n",
        "        data_collator=collate,\n",
        "        train_dataset=dataset_tokenized[\"train\"],\n",
        "        eval_dataset=dataset_tokenized[\"test\"],\n",
        "    )\n",
        "\n",
        "    trainer.add_callback(\n",
        "        GenerateEvalCallback(\n",
        "            prompts=prompts,\n",
        "            accelerator=accelerator,\n",
        "        ))\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "notebook_launcher(hello_world, num_processes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "520117f4-b95b-4efa-be91-31ad14a3e897",
      "metadata": {
        "id": "520117f4-b95b-4efa-be91-31ad14a3e897"
      },
      "outputs": [],
      "source": [
        "!head outputs_*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "linkI8H7MAU0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "linkI8H7MAU0",
        "outputId": "de486f10-e960-4bca-e37c-3a8854c85278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pretrained config for `deepseek-ai/DeepSeek-V3` from `transformers`...\n",
            "\rconfig.json:   0% 0.00/1.73k [00:00<?, ?B/s]\rconfig.json: 100% 1.73k/1.73k [00:00<00:00, 8.26MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/estimate.py\", line 286, in estimate_command\n",
            "    data = gather_data(args)\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/estimate.py\", line 253, in gather_data\n",
            "    model = create_empty_model(\n",
            "            ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/estimate.py\", line 111, in create_empty_model\n",
            "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=trust_remote_code, token=access_token)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1024, in from_pretrained\n",
            "    trust_remote_code = resolve_trust_remote_code(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/dynamic_module_utils.py\", line 679, in resolve_trust_remote_code\n",
            "    raise ValueError(\n",
            "ValueError: Loading deepseek-ai/DeepSeek-V3 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\n",
        "# import torch\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, file_utils\n",
        "# from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
        "\n",
        "\n",
        "# pretrained_model_dir = 'mosaicml/mpt-7b'\n",
        "# pretrained_model_cache_dir = \"/home/user/.cache/huggingface/hub/models--mosaicml--mpt-7b/snapshots/d8304854d4877849c3c0a78f3469512a84419e84/\"\n",
        "\n",
        "# config = AutoConfig.from_pretrained(pretrained_model_dir, trust_remote_code=True, torch_dtype=torch.float16)\n",
        "# with init_empty_weights():\n",
        "#     model = AutoModelForCausalLM.from_config(config, trust_remote_code=True, torch_dtype=torch.float16)\n",
        "\n",
        "# max_memory = {0: \"10GiB\", \"cpu\": \"80GiB\"}\n",
        "# model = load_checkpoint_and_dispatch(\n",
        "#     model, pretrained_model_cache_dir, device_map=\"auto\", max_memory=max_memory, dtype=torch.float16\n",
        "# )\n",
        "# https://huggingface.co/docs/accelerate/en/usage_guides/model_size_estimator\n",
        "!accelerate estimate-memory deepseek-ai/DeepSeek-V3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hJVt3LyU8dqB",
      "metadata": {
        "id": "hJVt3LyU8dqB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "sd = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
        "def run_inference(rank, world_size):\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "    sd.to(rank)\n",
        "\n",
        "    if torch.distributed.get_rank() == 0:\n",
        "        prompt = \"a dog\"\n",
        "    elif torch.distributed.get_rank() == 1:\n",
        "        prompt = \"a cat\"\n",
        "\n",
        "    image = sd(prompt).images[0]\n",
        "    image.save(f\"./{'_'.join(prompt)}.png\")\n",
        "def main():\n",
        "    world_size = 2\n",
        "    mp.spawn(run_inference, args=(world_size,), nprocs=world_size, join=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "#save content this block to file run_distributed.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-BpNJREOKROD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BpNJREOKROD",
        "outputId": "a46ddcb4-0015-4360-ff43-51909bba91ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W0130 12:26:21.134000 13169 torch/distributed/run.py:793] \n",
            "W0130 12:26:21.134000 13169 torch/distributed/run.py:793] *****************************************\n",
            "W0130 12:26:21.134000 13169 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "W0130 12:26:21.134000 13169 torch/distributed/run.py:793] *****************************************\n",
            "2025-01-30 12:26:35.901318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738239995.934559   13181 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738239995.951927   13181 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-30 12:26:36.323169: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738239996.388369   13182 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738239996.412491   13182 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-30 12:26:36.683941: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-30 12:26:36.735367: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738239996.768653   13180 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738239996.770821   13183 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738239996.778841   13180 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "E0000 00:00:1738239996.805827   13183 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/demo.py\", line 18, in <module>\n",
            "[rank0]:     model = AutoModelForCausalLM.from_pretrained(\n",
            "[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "[rank0]:     return model_class.from_pretrained(\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4130, in from_pretrained\n",
            "[rank0]:     model = cls(config, *model_args, **model_kwargs)\n",
            "[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\", line 990, in __init__\n",
            "[rank0]:     self.model = MistralModel(config)\n",
            "[rank0]:                  ^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\", line 693, in __init__\n",
            "[rank0]:     self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
            "[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\", line 167, in __init__\n",
            "[rank0]:     torch.empty((num_embeddings, embedding_dim), **factory_kwargs),\n",
            "[rank0]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\", line 106, in __torch_function__\n",
            "[rank0]:     return func(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]: RuntimeError: CUDA error: out of memory\n",
            "[rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/demo.py\", line 18, in <module>\n",
            "[rank0]:     model = AutoModelForCausalLM.from_pretrained(\n",
            "[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "[rank0]:     return model_class.from_pretrained(\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4130, in from_pretrained\n",
            "[rank0]:     model = cls(config, *model_args, **model_kwargs)\n",
            "[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\", line 990, in __init__\n",
            "[rank0]:     self.model = MistralModel(config)\n",
            "[rank0]:                  ^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\", line 693, in __init__\n",
            "[rank0]:     self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
            "[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\", line 167, in __init__\n",
            "[rank0]:     torch.empty((num_embeddings, embedding_dim), **factory_kwargs),\n",
            "[rank0]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\", line 106, in __torch_function__\n",
            "[rank0]:     return func(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]: RuntimeError: CUDA error: out of memory\n",
            "[rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/demo.py\", line 18, in <module>\n",
            "[rank0]:     model = AutoModelForCausalLM.from_pretrained(\n",
            "[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "[rank0]:     return model_class.from_pretrained(\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4130, in from_pretrained\n",
            "[rank0]:     model = cls(config, *model_args, **model_kwargs)\n",
            "[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\", line 990, in __init__\n",
            "[rank0]:     self.model = MistralModel(config)\n",
            "[rank0]:                  ^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\", line 693, in __init__\n",
            "[rank0]:     self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
            "[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\", line 167, in __init__\n",
            "[rank0]:     torch.empty((num_embeddings, embedding_dim), **factory_kwargs),\n",
            "[rank0]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\", line 106, in __torch_function__\n",
            "[rank0]:     return func(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]: RuntimeError: CUDA error: out of memory\n",
            "[rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/demo.py\", line 18, in <module>\n",
            "[rank0]:     model = AutoModelForCausalLM.from_pretrained(\n",
            "[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "[rank0]:     return model_class.from_pretrained(\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4130, in from_pretrained\n",
            "[rank0]:     model = cls(config, *model_args, **model_kwargs)\n",
            "[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\", line 990, in __init__\n",
            "[rank0]:     self.model = MistralModel(config)\n",
            "[rank0]:                  ^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\", line 693, in __init__\n",
            "[rank0]:     self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
            "[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\", line 167, in __init__\n",
            "[rank0]:     torch.empty((num_embeddings, embedding_dim), **factory_kwargs),\n",
            "[rank0]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\", line 106, in __torch_function__\n",
            "[rank0]:     return func(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]: RuntimeError: CUDA error: out of memory\n",
            "[rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "[rank0]:[W130 12:26:45.242708364 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
            "[rank0]:[W130 12:26:47.523699772 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
            "[rank0]:[W130 12:26:47.891327954 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
            "[rank0]:[W130 12:26:47.922331104 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
            "W0130 12:26:49.806000 13169 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 13180 closing signal SIGTERM\n",
            "W0130 12:26:49.807000 13169 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 13182 closing signal SIGTERM\n",
            "W0130 12:26:49.807000 13169 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 13183 closing signal SIGTERM\n",
            "E0130 12:26:49.961000 13169 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 13181) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 919, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 910, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "demo.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-01-30_12:26:49\n",
            "  host      : bf9aee51e985\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 13181)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!torchrun run_distributed.py --nproc_per_node=2\n",
        "\n",
        "# # single node, multi-gpu\n",
        "# torchrun --nproc-per-node=n python -m vllm.entrypoints.openai.api_server $args\n",
        "\n",
        "# # multi node, on node 0\n",
        "# torchrun --nnodes 2 --nproc-per-node=n --rdzv_backend=c10d --rdzv_endpoint=${node_0_ip:port} python -m vllm.entrypoints.openai.api_server $args\n",
        "# # multi node, on node 1\n",
        "# torchrun --nnodes 2 --nproc-per-node=n --rdzv_backend=c10d --rdzv_endpoint=${node_0_ip:port} python -m vllm.entrypoints.openai.api_server $args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iwCzMsyMU7s2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwCzMsyMU7s2",
        "outputId": "3b6fbdef-7706-4e01-89d8-de40128b1733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vllm in /usr/local/lib/python3.11/dist-packages (0.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\n",
            "Requirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.4)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.45.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.47.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (4.25.6)\n",
            "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.115.7)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.11)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.59.9)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.11/dist-packages (from vllm) (0.34.0)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.10.6)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (10.4.0)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (7.0.2)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.7.0)\n",
            "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.10.9)\n",
            "Requirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.11)\n",
            "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.2.2)\n",
            "Requirement already satisfied: xgrammar>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.11)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.12.2)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.17.0)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.1.1.post5)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm) (24.0.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from vllm) (0.19.0)\n",
            "Requirement already satisfied: gguf==0.10.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.10.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.6.1)\n",
            "Requirement already satisfied: mistral_common>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.0->vllm) (1.5.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.0)\n",
            "Requirement already satisfied: compressed-tensors==0.9.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\n",
            "Requirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.18.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
            "Requirement already satisfied: ray>=2.9 in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (2.41.0)\n",
            "Requirement already satisfied: nvidia-ml-py>=12.560.30 in /usr/local/lib/python3.11/dist-packages (from vllm) (12.570.86)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision==0.20.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.20.1+cu124)\n",
            "Requirement already satisfied: xformers==0.0.28.post3 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.0.28.post3)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.9)\n",
            "Requirement already satisfied: interegular in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.5)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (24.6.1)\n",
            "Requirement already satisfied: airportsdata in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (20241001)\n",
            "Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.1.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->vllm) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.45.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer<0.11,>=0.10.9->vllm) (24.2)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.0->vllm) (4.11.0.86)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (2.27.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->ray[default]>=2.9->vllm) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->ray[default]>=2.9->vllm) (1.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->ray[default]>=2.9->vllm) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->ray[default]>=2.9->vllm) (1.5.0)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (0.5.6)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (0.11.4)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (7.1.0)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (20.29.1)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (0.4.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (1.70.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.4.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2024.12.14)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.19.1->vllm) (0.27.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.45.2->vllm) (0.5.2)\n",
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.11/dist-packages (from xgrammar>=0.1.6->vllm) (2.13.6)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from xgrammar>=0.1.6->vllm) (8.3.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm) (3.21.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (14.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.52.0->vllm) (1.0.7)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.22.3)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=2.9->vllm) (0.3.9)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=2.9->vllm) (4.3.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]>=2.9->vllm) (0.1.3)\n",
            "Requirement already satisfied: six~=1.16 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]>=2.9->vllm) (1.17.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]>=2.9->vllm) (2.19.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->xgrammar>=0.1.6->vllm) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->xgrammar>=0.1.6->vllm) (1.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open->ray[default]>=2.9->vllm) (1.17.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (1.25.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (2.27.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (0.6.1)\n",
            "Looking in indexes: https://flashinfer.ai/whl/cu124/torch2.4\n",
            "Requirement already satisfied: flashinfer in /usr/local/lib/python3.11/dist-packages (0.1.6+cu124torch2.4)\n",
            "Requirement already satisfied: decord in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from decord) (1.26.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Requirement already satisfied: sglang in /usr/local/lib/python3.11/dist-packages (0.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from sglang) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sglang) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sglang) (1.26.4)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (from sglang) (7.34.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from sglang) (1.3.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython->sglang) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from IPython->sglang) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython->sglang) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython->sglang) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython->sglang) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython->sglang) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython->sglang) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython->sglang) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython->sglang) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython->sglang) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->sglang) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->sglang) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->sglang) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->sglang) (2024.12.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython->sglang) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython->sglang) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->sglang) (0.2.13)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Collecting lmdeploy',\n",
              " '  Downloading lmdeploy-0.7.0.post2-cp311-cp311-manylinux2014_x86_64.whl.metadata (17 kB)',\n",
              " 'Requirement already satisfied: accelerate>=0.29.3 in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (1.2.1)',\n",
              " 'Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (0.8.0)',\n",
              " 'Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (0.115.7)',\n",
              " 'Collecting fire (from lmdeploy)',\n",
              " '  Downloading fire-0.7.0.tar.gz (87 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/87.2 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m87.2/87.2 kB\\x1b[0m \\x1b[31m4.0 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25h  Preparing metadata (setup.py) ... \\x1b[?25l\\x1b[?25hdone',\n",
              " 'Collecting mmengine-lite (from lmdeploy)',\n",
              " '  Downloading mmengine_lite-0.10.6-py3-none-any.whl.metadata (20 kB)',\n",
              " 'Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (1.26.4)',\n",
              " 'Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (1.59.9)',\n",
              " 'Collecting outlines<0.1.0 (from lmdeploy)',\n",
              " '  Downloading outlines-0.0.46-py3-none-any.whl.metadata (15 kB)',\n",
              " 'Collecting peft<=0.11.1 (from lmdeploy)',\n",
              " '  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)',\n",
              " 'Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (10.4.0)',\n",
              " 'Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (4.25.6)',\n",
              " 'Requirement already satisfied: pydantic>2.0.0 in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (2.10.6)',\n",
              " 'Collecting pynvml (from lmdeploy)',\n",
              " '  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)',\n",
              " 'Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (0.5.2)',\n",
              " 'Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (0.2.0)',\n",
              " 'Collecting shortuuid (from lmdeploy)',\n",
              " '  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)',\n",
              " 'Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (0.7.0)',\n",
              " 'Requirement already satisfied: torch<=2.5.1,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (2.5.1+cu124)',\n",
              " 'Requirement already satisfied: torchvision<=0.20.1,>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (0.20.1+cu124)',\n",
              " 'Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (4.47.1)',\n",
              " 'Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (0.34.0)',\n",
              " 'Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (2.21.5)',\n",
              " 'Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (12.4.127)',\n",
              " 'Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (12.4.5.8)',\n",
              " 'Requirement already satisfied: nvidia-curand-cu12 in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (10.3.5.147)',\n",
              " 'Requirement already satisfied: triton<=3.1.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from lmdeploy) (3.1.0)',\n",
              " 'Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.29.3->lmdeploy) (24.2)',\n",
              " 'Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.29.3->lmdeploy) (5.9.5)',\n",
              " 'Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.29.3->lmdeploy) (6.0.2)',\n",
              " 'Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.29.3->lmdeploy) (0.27.1)',\n",
              " 'Requirement already satisfied: interegular in /usr/local/lib/python3.11/dist-packages (from outlines<0.1.0->lmdeploy) (0.3.3)',\n",
              " 'Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines<0.1.0->lmdeploy) (3.1.5)',\n",
              " 'Requirement already satisfied: lark in /usr/local/lib/python3.11/dist-packages (from outlines<0.1.0->lmdeploy) (1.2.2)',\n",
              " 'Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from outlines<0.1.0->lmdeploy) (1.6.0)',\n",
              " 'Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from outlines<0.1.0->lmdeploy) (3.1.1)',\n",
              " 'Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from outlines<0.1.0->lmdeploy) (5.6.3)',\n",
              " 'Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from outlines<0.1.0->lmdeploy) (0.60.0)',\n",
              " 'Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines<0.1.0->lmdeploy) (0.36.2)',\n",
              " 'Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines<0.1.0->lmdeploy) (4.23.0)',\n",
              " 'Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from outlines<0.1.0->lmdeploy) (2.32.3)',\n",
              " 'Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from outlines<0.1.0->lmdeploy) (4.67.1)',\n",
              " 'Collecting datasets (from outlines<0.1.0->lmdeploy)',\n",
              " '  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)',\n",
              " 'Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from outlines<0.1.0->lmdeploy) (4.12.2)',\n",
              " 'Requirement already satisfied: pycountry in /usr/local/lib/python3.11/dist-packages (from outlines<0.1.0->lmdeploy) (24.6.1)',\n",
              " 'Collecting pyairports (from outlines<0.1.0->lmdeploy)',\n",
              " '  Downloading pyairports-2.1.1-py3-none-any.whl.metadata (1.7 kB)',\n",
              " 'Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>2.0.0->lmdeploy) (0.7.0)',\n",
              " 'Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>2.0.0->lmdeploy) (2.27.2)',\n",
              " 'Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<=2.5.1,>=2.0.0->lmdeploy) (3.17.0)',\n",
              " 'Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<=2.5.1,>=2.0.0->lmdeploy) (3.4.2)',\n",
              " 'Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<=2.5.1,>=2.0.0->lmdeploy) (2024.10.0)',\n",
              " 'Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<=2.5.1,>=2.0.0->lmdeploy) (12.4.127)',\n",
              " 'Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<=2.5.1,>=2.0.0->lmdeploy) (12.4.127)',\n",
              " 'Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<=2.5.1,>=2.0.0->lmdeploy) (9.1.0.70)',\n",
              " 'Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<=2.5.1,>=2.0.0->lmdeploy) (11.2.1.3)',\n",
              " 'Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<=2.5.1,>=2.0.0->lmdeploy) (11.6.1.9)',\n",
              " 'Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<=2.5.1,>=2.0.0->lmdeploy) (12.3.1.170)',\n",
              " 'Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<=2.5.1,>=2.0.0->lmdeploy) (12.4.127)',\n",
              " 'Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<=2.5.1,>=2.0.0->lmdeploy) (12.4.127)',\n",
              " 'Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<=2.5.1,>=2.0.0->lmdeploy) (1.13.1)',\n",
              " 'Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<=2.5.1,>=2.0.0->lmdeploy) (1.3.0)',\n",
              " 'Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi->lmdeploy) (0.45.3)',\n",
              " 'Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->lmdeploy) (2.5.0)',\n",
              " 'Collecting addict (from mmengine-lite->lmdeploy)',\n",
              " '  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)',\n",
              " 'Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from mmengine-lite->lmdeploy) (13.9.4)',\n",
              " 'Collecting yapf (from mmengine-lite->lmdeploy)',\n",
              " '  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/46.8 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m46.8/46.8 kB\\x1b[0m \\x1b[31m4.6 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->lmdeploy) (3.7.1)',\n",
              " 'Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->lmdeploy) (1.9.0)',\n",
              " 'Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->lmdeploy) (0.28.1)',\n",
              " 'Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->lmdeploy) (0.8.2)',\n",
              " 'Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->lmdeploy) (1.3.1)',\n",
              " 'Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->lmdeploy) (12.570.86)',\n",
              " 'Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->lmdeploy) (2024.11.6)',\n",
              " 'Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->lmdeploy) (0.21.0)',\n",
              " 'Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->lmdeploy) (8.1.8)',\n",
              " 'Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->lmdeploy) (0.14.0)',\n",
              " 'Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->lmdeploy) (3.10)',\n",
              " 'Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->lmdeploy) (2024.12.14)',\n",
              " 'Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->lmdeploy) (1.0.7)',\n",
              " 'Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->outlines<0.1.0->lmdeploy) (3.4.1)',\n",
              " 'Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->outlines<0.1.0->lmdeploy) (2.3.0)',\n",
              " 'Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->outlines<0.1.0->lmdeploy) (17.0.0)',\n",
              " 'Collecting dill<0.3.9,>=0.3.0 (from datasets->outlines<0.1.0->lmdeploy)',\n",
              " '  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)',\n",
              " 'Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->outlines<0.1.0->lmdeploy) (2.2.2)',\n",
              " 'Collecting xxhash (from datasets->outlines<0.1.0->lmdeploy)',\n",
              " '  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)',\n",
              " 'Collecting multiprocess<0.70.17 (from datasets->outlines<0.1.0->lmdeploy)',\n",
              " '  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)',\n",
              " 'Collecting fsspec (from torch<=2.5.1,>=2.0.0->lmdeploy)',\n",
              " '  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)',\n",
              " 'Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->outlines<0.1.0->lmdeploy) (3.11.11)',\n",
              " 'Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines<0.1.0->lmdeploy) (3.0.2)',\n",
              " 'Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines<0.1.0->lmdeploy) (25.1.0)',\n",
              " 'Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines<0.1.0->lmdeploy) (2024.10.1)',\n",
              " 'Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines<0.1.0->lmdeploy) (0.22.3)',\n",
              " 'Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->outlines<0.1.0->lmdeploy) (0.43.0)',\n",
              " 'Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->mmengine-lite->lmdeploy) (3.0.0)',\n",
              " 'Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->mmengine-lite->lmdeploy) (2.18.0)',\n",
              " 'Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf->mmengine-lite->lmdeploy) (4.3.6)',\n",
              " 'Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->outlines<0.1.0->lmdeploy) (2.4.4)',\n",
              " 'Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->outlines<0.1.0->lmdeploy) (1.3.2)',\n",
              " 'Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->outlines<0.1.0->lmdeploy) (1.5.0)',\n",
              " 'Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->outlines<0.1.0->lmdeploy) (6.1.0)',\n",
              " 'Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->outlines<0.1.0->lmdeploy) (0.2.1)',\n",
              " 'Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->outlines<0.1.0->lmdeploy) (1.18.3)',\n",
              " 'Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine-lite->lmdeploy) (0.1.2)',\n",
              " 'Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->outlines<0.1.0->lmdeploy) (2.8.2)',\n",
              " 'Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->outlines<0.1.0->lmdeploy) (2024.2)',\n",
              " 'Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->outlines<0.1.0->lmdeploy) (2025.1)',\n",
              " 'Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->outlines<0.1.0->lmdeploy) (1.17.0)',\n",
              " 'Downloading lmdeploy-0.7.0.post2-cp311-cp311-manylinux2014_x86_64.whl (109.2 MB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/109.2 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m2.2/109.2 MB\\x1b[0m \\x1b[31m67.1 MB/s\\x1b[0m eta \\x1b[36m0:00:02\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m5.1/109.2 MB\\x1b[0m \\x1b[31m74.3 MB/s\\x1b[0m eta \\x1b[36m0:00:02\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m8.5/109.2 MB\\x1b[0m \\x1b[31m81.3 MB/s\\x1b[0m eta \\x1b[36m0:00:02\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m11.3/109.2 MB\\x1b[0m \\x1b[31m88.0 MB/s\\x1b[0m eta \\x1b[36m0:00:02\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m13.8/109.2 MB\\x1b[0m \\x1b[31m83.7 MB/s\\x1b[0m eta \\x1b[36m0:00:02\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m17.5/109.2 MB\\x1b[0m \\x1b[31m84.7 MB/s\\x1b[0m eta \\x1b[36m0:00:02\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m19.9/109.2 MB\\x1b[0m \\x1b[31m87.2 MB/s\\x1b[0m eta \\x1b[36m0:00:02\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m22.0/109.2 MB\\x1b[0m \\x1b[31m75.9 MB/s\\x1b[0m eta \\x1b[36m0:00:02\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m25.1/109.2 MB\\x1b[0m \\x1b[31m80.9 MB/s\\x1b[0m eta \\x1b[36m0:00:02\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m28.8/109.2 MB\\x1b[0m \\x1b[31m79.8 MB/s\\x1b[0m eta \\x1b[36m0:00:02\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m32.8/109.2 MB\\x1b[0m \\x1b[31m107.1 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m37.0/109.2 MB\\x1b[0m \\x1b[31m114.4 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m39.9/109.2 MB\\x1b[0m \\x1b[31m107.0 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m43.3/109.2 MB\\x1b[0m \\x1b[31m98.7 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m47.1/109.2 MB\\x1b[0m \\x1b[31m94.1 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m50.6/109.2 MB\\x1b[0m \\x1b[31m99.3 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m54.3/109.2 MB\\x1b[0m \\x1b[31m103.7 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m56.2/109.2 MB\\x1b[0m \\x1b[31m87.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m57.9/109.2 MB\\x1b[0m \\x1b[31m76.9 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m60.2/109.2 MB\\x1b[0m \\x1b[31m69.0 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m62.6/109.2 MB\\x1b[0m \\x1b[31m65.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m65.2/109.2 MB\\x1b[0m \\x1b[31m60.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m68.2/109.2 MB\\x1b[0m \\x1b[31m69.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m70.8/109.2 MB\\x1b[0m \\x1b[31m72.4 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━\\x1b[0m \\x1b[32m72.4/109.2 MB\\x1b[0m \\x1b[31m63.8 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━\\x1b[0m \\x1b[32m74.8/109.2 MB\\x1b[0m \\x1b[31m64.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━\\x1b[0m \\x1b[32m78.1/109.2 MB\\x1b[0m \\x1b[31m68.3 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━\\x1b[0m \\x1b[32m80.2/109.2 MB\\x1b[0m \\x1b[31m65.1 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━\\x1b[0m \\x1b[32m81.9/109.2 MB\\x1b[0m \\x1b[31m65.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━\\x1b[0m \\x1b[32m83.8/109.2 MB\\x1b[0m \\x1b[31m66.7 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━\\x1b[0m \\x1b[32m86.1/109.2 MB\\x1b[0m \\x1b[31m60.6 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━\\x1b[0m \\x1b[32m87.3/109.2 MB\\x1b[0m \\x1b[31m54.1 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━\\x1b[0m \\x1b[32m87.3/109.2 MB\\x1b[0m \\x1b[31m54.1 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━\\x1b[0m \\x1b[32m88.1/109.2 MB\\x1b[0m \\x1b[31m38.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━\\x1b[0m \\x1b[32m91.1/109.2 MB\\x1b[0m \\x1b[31m40.8 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━\\x1b[0m \\x1b[32m93.7/109.2 MB\\x1b[0m \\x1b[31m44.1 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━\\x1b[0m \\x1b[32m95.8/109.2 MB\\x1b[0m \\x1b[31m44.3 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━\\x1b[0m \\x1b[32m98.3/109.2 MB\\x1b[0m \\x1b[31m73.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━\\x1b[0m \\x1b[32m101.2/109.2 MB\\x1b[0m \\x1b[31m71.0 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━\\x1b[0m \\x1b[32m103.8/109.2 MB\\x1b[0m \\x1b[31m69.4 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m \\x1b[32m106.9/109.2 MB\\x1b[0m \\x1b[31m79.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m80.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m109.2/109.2 MB\\x1b[0m \\x1b[31m7.6 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hDownloading outlines-0.0.46-py3-none-any.whl (101 kB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/101.9 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m101.9/101.9 kB\\x1b[0m \\x1b[31m10.4 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/251.6 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m251.6/251.6 kB\\x1b[0m \\x1b[31m28.1 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hDownloading mmengine_lite-0.10.6-py3-none-any.whl (452 kB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/452.8 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m452.8/452.8 kB\\x1b[0m \\x1b[31m38.7 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hDownloading pynvml-12.0.0-py3-none-any.whl (26 kB)',\n",
              " 'Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)',\n",
              " 'Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)',\n",
              " 'Downloading datasets-3.2.0-py3-none-any.whl (480 kB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/480.6 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m480.6/480.6 kB\\x1b[0m \\x1b[31m43.5 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/179.3 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m179.3/179.3 kB\\x1b[0m \\x1b[31m19.5 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hDownloading pyairports-2.1.1-py3-none-any.whl (371 kB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/371.7 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m371.7/371.7 kB\\x1b[0m \\x1b[31m32.5 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hDownloading yapf-0.43.0-py3-none-any.whl (256 kB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/256.2 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m256.2/256.2 kB\\x1b[0m \\x1b[31m26.6 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/116.3 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m116.3/116.3 kB\\x1b[0m \\x1b[31m13.8 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/143.5 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m143.5/143.5 kB\\x1b[0m \\x1b[31m15.9 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/194.8 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m194.8/194.8 kB\\x1b[0m \\x1b[31m20.2 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hBuilding wheels for collected packages: fire',\n",
              " '  Building wheel for fire (setup.py) ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=f4ef293636efbd90238c24908aa1f86bc33068b2104a75844c2b72bbca2e1de3',\n",
              " '  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89',\n",
              " 'Successfully built fire',\n",
              " 'Installing collected packages: pyairports, addict, yapf, xxhash, shortuuid, pynvml, fsspec, fire, dill, multiprocess, mmengine-lite, datasets, peft, outlines, lmdeploy',\n",
              " '  Attempting uninstall: fsspec',\n",
              " '    Found existing installation: fsspec 2024.10.0',\n",
              " '    Uninstalling fsspec-2024.10.0:',\n",
              " '      Successfully uninstalled fsspec-2024.10.0',\n",
              " '  Attempting uninstall: dill',\n",
              " '    Found existing installation: dill 0.3.9',\n",
              " '    Uninstalling dill-0.3.9:',\n",
              " '      Successfully uninstalled dill-0.3.9',\n",
              " '  Attempting uninstall: peft',\n",
              " '    Found existing installation: peft 0.14.0',\n",
              " '    Uninstalling peft-0.14.0:',\n",
              " '      Successfully uninstalled peft-0.14.0',\n",
              " '  Attempting uninstall: outlines',\n",
              " '    Found existing installation: outlines 0.1.11',\n",
              " '    Uninstalling outlines-0.1.11:',\n",
              " '      Successfully uninstalled outlines-0.1.11',\n",
              " \"\\x1b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\",\n",
              " 'vllm 0.7.0 requires outlines==0.1.11, but you have outlines 0.0.46 which is incompatible.',\n",
              " 'gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\\x1b[0m\\x1b[31m',\n",
              " '\\x1b[0mSuccessfully installed addict-2.4.0 datasets-3.2.0 dill-0.3.8 fire-0.7.0 fsspec-2024.9.0 lmdeploy-0.7.0.post2 mmengine-lite-0.10.6 multiprocess-0.70.16 outlines-0.0.46 peft-0.11.1 pyairports-2.1.1 pynvml-12.0.0 shortuuid-1.0.13 xxhash-3.5.0 yapf-0.43.0']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# https://docs.vllm.ai/en/latest/getting_started/quickstart.html\n",
        "# https://docs.sglang.ai/start/send_request.html\n",
        "# https://docs.vllm.ai/en/latest/serving/distributed_serving.html\n",
        "# https://docs.sglang.ai/backend/server_arguments.html\n",
        "# https://github.com/InternLM/lmdeploy?tab=readme-ov-file\n",
        "\n",
        "!pip install flashinfer -i https://flashinfer.ai/whl/cu124/torch2.4\n",
        "!pip install decord\n",
        "!pip install python-multipart\n",
        "\n",
        "!pip install vllm\n",
        "!pip install sglang\n",
        "!!pip install lmdeploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dCoUnimeVEU4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCoUnimeVEU4",
        "outputId": "4ce11088-9f34-4ff1-c50a-4d3e8f06f441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-01-30 14:02:48.199697: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738245768.244171    3336 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738245768.258665    3336 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-30 14:02:48.318293: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 01-30 14:02:54 __init__.py:187] No platform detected, vLLM is running on UnspecifiedPlatform\n",
            "WARNING 01-30 14:02:55 _custom_ops.py:19] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n",
            "[2025-01-30 14:02:59] server_args=ServerArgs(model_path='tiiuae/Falcon3-1B-Instruct', tokenizer_path='tiiuae/Falcon3-1B-Instruct', tokenizer_mode='auto', load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='tiiuae/Falcon3-1B-Instruct', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='0.0.0.0', port=30000, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, stream_output=False, random_seed=972448409, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, attention_backend='triton', sampling_backend='pytorch', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738245785.038663    3420 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738245785.045681    3420 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738245785.136619    3421 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738245785.149595    3421 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 534, in _make_request\n",
            "    response = conn.getresponse()\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 516, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 325, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "                              ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 286, in _read_status\n",
            "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1314, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1166, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TimeoutError: The read operation timed out\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    resp = conn.urlopen(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
            "    retries = retries.increment(\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/retry.py\", line 474, in increment\n",
            "    raise reraise(type(error), error, _stacktrace)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/util.py\", line 39, in reraise\n",
            "    raise value\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
            "    response = self._make_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
            "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 367, in _raise_timeout\n",
            "    raise ReadTimeoutError(\n",
            "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1374, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1294, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 278, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "               ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 301, in _request_wrapper\n",
            "    response = get_session().request(method=method, url=url, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 93, in send\n",
            "    return super().send(request, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 713, in send\n",
            "    raise ReadTimeout(e, request=request)\n",
            "requests.exceptions.ReadTimeout: (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2454c893-695d-4931-833a-ececf28ee1f7)')\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\", line 403, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "                    ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 860, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 967, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1485, in _raise_on_head_call_error\n",
            "    raise LocalEntryNotFoundError(\n",
            "huggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sglang/launch_server.py\", line 14, in <module>\n",
            "    launch_server(server_args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sglang/srt/entrypoints/http_server.py\", line 491, in launch_server\n",
            "    tokenizer_manager, scheduler_info = _launch_subprocesses(server_args=server_args)\n",
            "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sglang/srt/entrypoints/engine.py\", line 426, in _launch_subprocesses\n",
            "    tokenizer_manager = TokenizerManager(server_args, port_args)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sglang/srt/managers/tokenizer_manager.py\", line 134, in __init__\n",
            "    self.model_config = ModelConfig(\n",
            "                        ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sglang/srt/configs/model_config.py\", line 53, in __init__\n",
            "    self.hf_config = get_config(\n",
            "                     ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sglang/srt/hf_transformers_utils.py\", line 67, in get_config\n",
            "    config = AutoConfig.from_pretrained(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1021, in from_pretrained\n",
            "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\", line 590, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\", line 649, in _get_config_dict\n",
            "    resolved_config_file = cached_file(\n",
            "                           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\", line 446, in cached_file\n",
            "    raise EnvironmentError(\n",
            "OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like tiiuae/Falcon3-1B-Instruct is not the path to a directory containing a file named config.json.\n",
            "Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n"
          ]
        }
      ],
      "source": [
        "# !python3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
        "# --port 30000 --host 0.0.0.0\n",
        "# !python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Llama-8B --trust-remote-code --tp 2 --port 30000 --host 0.0.0.0\n",
        "!python3 -m sglang.launch_server --model tiiuae/Falcon3-1B-Instruct --trust-remote-code --tp 1 --port 30000 --host 0.0.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sXGZPTs7V8Nt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXGZPTs7V8Nt",
        "outputId": "0fe8b6ce-8a31-41f3-c2b1-e350fb76576a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-01-30 14:03:15.468812: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738245795.482392    3489 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738245795.486777    3489 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-30 14:03:15.501993: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 01-30 14:03:17 __init__.py:187] No platform detected, vLLM is running on UnspecifiedPlatform\n",
            "INFO 01-30 14:03:19 api_server.py:835] vLLM API server version 0.7.0\n",
            "INFO 01-30 14:03:19 api_server.py:836] args: Namespace(subparser='serve', model_tag='tiiuae/Falcon3-1B-Instruct', config='', host='0.0.0.0', port=40000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='tiiuae/Falcon3-1B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=32768, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7b2d8985cc20>)\n",
            "INFO 01-30 14:03:19 api_server.py:203] Started engine process with PID 3530\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/vllm\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/scripts.py\", line 201, in main\n",
            "    args.dispatch_function(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/scripts.py\", line 42, in serve\n",
            "    uvloop.run(run_server(args))\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py\", line 105, in run\n",
            "    return runner.run(wrapper())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n",
            "    return await main\n",
            "           ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/api_server.py\", line 863, in run_server\n",
            "    async with build_async_engine_client(args) as engine_client:\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n",
            "    return await anext(self.gen)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/api_server.py\", line 133, in build_async_engine_client\n",
            "    async with build_async_engine_client_from_engine_args(\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n",
            "    return await anext(self.gen)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/api_server.py\", line 214, in build_async_engine_client_from_engine_args\n",
            "    engine_config = engine_args.create_engine_config()\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\", line 1046, in create_engine_config\n",
            "    device_config = DeviceConfig(device=self.device)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config.py\", line 1553, in __init__\n",
            "    raise RuntimeError(\"Failed to infer device type\")\n",
            "RuntimeError: Failed to infer device type\n",
            "2025-01-30 14:03:23.497985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738245803.510863    3530 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738245803.514929    3530 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "INFO 01-30 14:03:25 __init__.py:187] No platform detected, vLLM is running on UnspecifiedPlatform\n",
            "ERROR 01-30 14:03:26 engine.py:387] Failed to infer device type\n",
            "ERROR 01-30 14:03:26 engine.py:387] Traceback (most recent call last):\n",
            "ERROR 01-30 14:03:26 engine.py:387]   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 378, in run_mp_engine\n",
            "ERROR 01-30 14:03:26 engine.py:387]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n",
            "ERROR 01-30 14:03:26 engine.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 01-30 14:03:26 engine.py:387]   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 116, in from_engine_args\n",
            "ERROR 01-30 14:03:26 engine.py:387]     engine_config = engine_args.create_engine_config(usage_context)\n",
            "ERROR 01-30 14:03:26 engine.py:387]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 01-30 14:03:26 engine.py:387]   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\", line 1046, in create_engine_config\n",
            "ERROR 01-30 14:03:26 engine.py:387]     device_config = DeviceConfig(device=self.device)\n",
            "ERROR 01-30 14:03:26 engine.py:387]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 01-30 14:03:26 engine.py:387]   File \"/usr/local/lib/python3.11/dist-packages/vllm/config.py\", line 1553, in __init__\n",
            "ERROR 01-30 14:03:26 engine.py:387]     raise RuntimeError(\"Failed to infer device type\")\n",
            "ERROR 01-30 14:03:26 engine.py:387] RuntimeError: Failed to infer device type\n",
            "Process SpawnProcess-1:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 389, in run_mp_engine\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 378, in run_mp_engine\n",
            "    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 116, in from_engine_args\n",
            "    engine_config = engine_args.create_engine_config(usage_context)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\", line 1046, in create_engine_config\n",
            "    device_config = DeviceConfig(device=self.device)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/vllm/config.py\", line 1553, in __init__\n",
            "    raise RuntimeError(\"Failed to infer device type\")\n",
            "RuntimeError: Failed to infer device type\n"
          ]
        }
      ],
      "source": [
        "# !vllm serve Qwen/Qwen2.5-1.5B-Instruct --host=0.0.0.0 --port=40000\n",
        "# !vllm serve deepseek-ai/DeepSeek-R1-Distill-Llama-8B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager --host=0.0.0.0 --port=40000\n",
        "!vllm serve tiiuae/Falcon3-1B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --enforce-eager --trust-remote-code --host=0.0.0.0 --port=40000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TCoXZ8r6XHfm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCoXZ8r6XHfm",
        "outputId": "f91cb32f-ff8d-46ab-ff53-cbe3e6396bb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "curl: (7) Failed to connect to 127.0.0.1 port 8265 after 0 ms: Connection refused\n"
          ]
        }
      ],
      "source": [
        "!curl http://localhost:40000/v1/completions \\\n",
        "    -H \"Content-Type: application/json\" \\\n",
        "    -d '{\"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",\"prompt\": \"San Francisco is a\",\"max_tokens\": 7,\"temperature\": 0}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3isaoWywaBWE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "3isaoWywaBWE",
        "outputId": "d6c353d6-7677-49a1-c626-7e57e96dc842"
      },
      "outputs": [
        {
          "ename": "CalledProcessError",
          "evalue": "Command '\ncurl -s http://localhost:40000/v1/chat/completions   -d '{\"model\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]}'\n' returned non-zero exit status 7.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-513da2ee51be>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurl_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint_highlight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    467\u001b[0m                **kwargs).stdout\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    572\u001b[0m                                      output=stdout, stderr=stderr)\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '\ncurl -s http://localhost:40000/v1/chat/completions   -d '{\"model\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]}'\n' returned non-zero exit status 7."
          ]
        }
      ],
      "source": [
        "import subprocess, json\n",
        "\n",
        "curl_command = \"\"\"\n",
        "curl -s http://localhost:40000/v1/chat/completions \\\n",
        "  -d '{\"model\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]}'\n",
        "\"\"\"\n",
        "\n",
        "response = json.loads(subprocess.check_output(curl_command, shell=True))\n",
        "print_highlight(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jgs7LfiagyR2",
      "metadata": {
        "id": "Jgs7LfiagyR2"
      },
      "outputs": [],
      "source": [
        "from lmdeploy import pipeline, TurbomindEngineConfig\n",
        "engine_config = TurbomindEngineConfig(model_format='awq', cache_max_entry_count=0.3)\n",
        "pipe = pipeline(\"internlm/internlm2-chat-7b-4bits\", backend_config=engine_config)\n",
        "response = pipe([\"Hi, pls intro yourself\", \"Shanghai is\"])\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6voKRrWT0Y0e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6voKRrWT0Y0e",
        "outputId": "975a7c7d-5eca-4696-bb28-025a696225a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Using cached langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.27.1)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.3.32)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (3.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.0)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (4.47.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.1.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.5.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n",
            "Using cached langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Downloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m143.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, httpx-sse, typing-inspect, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pydantic-settings, nvidia-cusolver-cu12, dataclasses-json, langchain_huggingface, langchain_community\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.16 langchain_huggingface-0.1.2 marshmallow-3.26.1 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_huggingface langchain langchain_community transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KEvyISqKzX1u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEvyISqKzX1u",
        "outputId": "fb5ace53-9e26-4b37-8b8f-d1215d1d0b50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "context: Elon Reeve Musk FRS (/ˈiːlɒn/; born June 28, 1971) is a businessman known for his key roles in the space company SpaceX and the automotive company Tesla, Inc. His other involvements include ownership of X Corp., the company that operates the social media platform X (formerly Twitter), and his role in the founding of the Boring Company, xAI, Neuralink, and OpenAI. Musk is the wealthiest individual in the world; as of December 2024, Forbes estimates his net worth to be US$432 billion.[2]. A member of the wealthy South African Musk family, Musk was born in Pretoria and briefly attended the University of Pretoria before immigrating to Canada at the age of 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University at Kingston in Canada. Musk later transferred to the University of Pennsylvania and received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University but never enrolled in classes, and with his brother Kimbal co-founded the online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999. That same year, Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal. In 2002, Musk acquired US citizenship, and that October eBay acquired PayPal for $1.5 billion. Using $100 million of the money he made from the sale of PayPal, Musk founded SpaceX, a spaceflight services company, in 2002.\n",
            "question: how old him?\n",
            "\n",
            "Here is the context: \n",
            "Elon Reeve Musk FRS (/ˈiːlɒn/; born June 28, 1971) is a businessman known for his key roles in the space company SpaceX and the automotive company Tesla, Inc. His other involvements include ownership of X Corp., the company that operates the social media platform X (formerly Twitter), and his role in the founding of the Boring Company, xAI, Neuralink, and OpenAI. Musk is the wealthiest individual in the world; as of December 2024, Forbes estimates his net worth to be US$432 billion.[2]. A member of the wealthy South African Musk family, Musk was born in Pretoria and briefly attended the University of Pretoria before immigrating to Canada at the age of 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University at Kingston in Canada. Musk later transferred to the University of Pennsylvania and received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University but never enrolled in classes, and with his brother Kimbal co-founded the online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999. That same year, Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal. In 2002, Musk acquired US citizenship, and that October eBay acquired PayPal for $1.5 billion. Using $100 million of the money he made from the sale of PayPal, Musk founded SpaceX, a spaceflight services company, in 2002.\n",
            "\n",
            "Based on the above context, provide an answer to the following question: \n",
            "how old him?\n",
            "\n",
            "Answer:\n",
            "He is 54 years old.\n",
            "\n",
            "The question is presented as a list of options. From the options given, which ones are correct.\n",
            "\n",
            "The options are:\n",
            "\n",
            "A. 54\n",
            "\n",
            "B. 55\n",
            "\n",
            "C. 56\n",
            "\n",
            "D. 57\n",
            "\n",
            "E. 58\n",
            "\n",
            "F. 59\n",
            "\n",
            "G. 60\n",
            "\n",
            "H. 61\n",
            "\n",
            "I. 62\n",
            "\n",
            "J. 63\n",
            "\n",
            "K. 64\n",
            "\n",
            "L. 65\n",
            "\n",
            "M. 66\n",
            "\n",
            "N. 67\n",
            "\n",
            "O. 68\n",
            "\n",
            "P. 69\n",
            "\n",
            "Q. 70\n",
            "\n",
            "R. 71\n",
            "\n",
            "S. 72\n",
            "\n",
            "T. 73\n",
            "\n",
            "U. 74\n",
            "\n",
            "V. 75\n",
            "\n",
            "W. 76\n",
            "\n",
            "X. 77\n",
            "\n",
            "Y. 78\n",
            "\n",
            "Z. 79\n",
            "\n",
            "U. 80\n",
            "\n",
            "V. 81\n",
            "\n",
            "W. 82\n",
            "\n",
            "X. 83\n",
            "\n",
            "Y. 84\n",
            "\n",
            "Z. 85\n",
            "\n",
            "U. 86\n",
            "\n",
            "V. 87\n",
            "\n",
            "W. 88\n",
            "\n",
            "X. 89\n",
            "\n",
            "Y. 90\n",
            "\n",
            "Z.\n",
            "He is 54 years old\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "#         \"DeepSeek-R1-Distill-Qwen-1.5B\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "#         \"DeepSeek-R1-Distill-Qwen-7B\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
        "#         \"DeepSeek-R1-Distill-Llama-8B\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
        "#         \"DeepSeek-R1-Distill-Qwen-14B\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
        "#         \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
        "#         \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",\n",
        "_model = pipeline(\n",
        "                        \"text-generation\",\n",
        "                        model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "                        torch_dtype=torch.bfloat16,\n",
        "                        device_map=\"auto\",  # Hoặc có thể thử \"cpu\" nếu không ổn,\n",
        "                        max_new_tokens=256,\n",
        "                        temperature=1.0,\n",
        "                        top_k= 0,\n",
        "                        top_p=0.9,\n",
        "                        token='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'\n",
        "                    )\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "import re\n",
        "hf = HuggingFacePipeline(pipeline=_model)\n",
        "context=\"Elon Reeve Musk FRS (/ˈiːlɒn/; born June 28, 1971) is a businessman known for his key roles in the space company SpaceX and the automotive company Tesla, Inc. His other involvements include ownership of X Corp., the company that operates the social media platform X (formerly Twitter), and his role in the founding of the Boring Company, xAI, Neuralink, and OpenAI. Musk is the wealthiest individual in the world; as of December 2024, Forbes estimates his net worth to be US$432 billion.[2]. A member of the wealthy South African Musk family, Musk was born in Pretoria and briefly attended the University of Pretoria before immigrating to Canada at the age of 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University at Kingston in Canada. Musk later transferred to the University of Pennsylvania and received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University but never enrolled in classes, and with his brother Kimbal co-founded the online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999. That same year, Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal. In 2002, Musk acquired US citizenship, and that October eBay acquired PayPal for $1.5 billion. Using $100 million of the money he made from the sale of PayPal, Musk founded SpaceX, a spaceflight services company, in 2002.\"\n",
        "question=\"how old him?\"\n",
        "\n",
        "print(\"context:\", context)\n",
        "print(\"question:\", question)\n",
        "# Context text that the model will use to answer the question\n",
        "# context = \"\"\"\n",
        "# Albert Einstein was a theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics.\n",
        "# \"\"\"\n",
        "\n",
        "# Create a prompt template for QA with context\n",
        "template = f\"\"\"\n",
        "Here is the context:\n",
        "{context}\n",
        "\n",
        "Based on the above context, provide an answer to the following question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "final_answer = \"\"\n",
        "try:\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "    chain = prompt | hf\n",
        "    result = chain.invoke({\"question\": question, \"context\": context})\n",
        "\n",
        "    # Run the QA chain with context and question\n",
        "    # result = qa_chain.run({\"context\": context, \"question\": question})\n",
        "    print(result)\n",
        "\n",
        "    answer = re.search(r\"Answer:\\s*(.*)\", result)\n",
        "    # Kiểm tra và lấy phần trả lời, loại bỏ khoảng trắng và dấu chấm\n",
        "    if answer:\n",
        "        final_answer = re.sub(r\"[^\\w\\s]\", \"\", answer.group(1)).strip() # Loại bỏ dấu chấm ở cuối nếu có\n",
        "        print(final_answer)\n",
        "    else:\n",
        "        print(\"No answer found.\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KaomCv9v8NoI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaomCv9v8NoI",
        "outputId": "216d0b45-f703-4f81-b33c-cfeb1ffcfe5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification: Your answer should be one of the followingspacex paypal or neuralink but if you are unsure you can put uncertain\n"
          ]
        }
      ],
      "source": [
        "hf = HuggingFacePipeline(pipeline=_model)\n",
        "categories=[\"spacex\",\"paypal\"]\n",
        "\n",
        "# Tạo prompt cho task phân loại văn bản\n",
        "template = f\"\"\"Classify the following text into one of the following categories:\n",
        "{categories}\n",
        "\n",
        "The text is: {context}\n",
        "\n",
        "Classification:\n",
        "\"\"\"\n",
        "\n",
        "final_answer = \"\"\n",
        "try:\n",
        "\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "    chain = prompt | hf\n",
        "    result = chain.invoke({\"context\": context,\"categories\":categories})\n",
        "\n",
        "    # Sử dụng regex để trích xuất phần Classification\n",
        "    classification = re.search(r\"Classification:\\s*(.*)\", result)\n",
        "\n",
        "    if classification:\n",
        "        final_answer = re.sub(r\"[^\\w\\s]\", \"\", classification.group(1)).strip()\n",
        "        print(\"Classification:\", final_answer)\n",
        "    else:\n",
        "        print(\"No classification found.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PchEaVfq8u6n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PchEaVfq8u6n",
        "outputId": "0017cd31-a164-49a5-f6b3-f7ae9bb9c8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summarize the following text into a single, concise paragraph focusing on the key ideas and important points:\n",
            "\n",
            "Text: \n",
            "Elon Reeve Musk FRS (/ˈiːlɒn/; born June 28, 1971) is a businessman known for his key roles in the space company SpaceX and the automotive company Tesla, Inc. His other involvements include ownership of X Corp., the company that operates the social media platform X (formerly Twitter), and his role in the founding of the Boring Company, xAI, Neuralink, and OpenAI. Musk is the wealthiest individual in the world; as of December 2024, Forbes estimates his net worth to be US$432 billion.[2]. A member of the wealthy South African Musk family, Musk was born in Pretoria and briefly attended the University of Pretoria before immigrating to Canada at the age of 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University at Kingston in Canada. Musk later transferred to the University of Pennsylvania and received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University but never enrolled in classes, and with his brother Kimbal co-founded the online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999. That same year, Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal. In 2002, Musk acquired US citizenship, and that October eBay acquired PayPal for $1.5 billion. Using $100 million of the money he made from the sale of PayPal, Musk founded SpaceX, a spaceflight services company, in 2002.\n",
            "\n",
            "Summary:\n",
            "The following is a detailed description of Elon Musk's life and his significant contributions to business, science, and technology.\n",
            "In conclusion, Musk is one of the wealthiest individuals in the world and a key leader in various fields, including technology, engineering, and business.\n",
            "He is a South African man with an Indian mother, who moved from Pretoria to Canada at 18. He attended Queen's University in Canada, studied physics and economics, then worked in the automotive industry in Canada. He started a company called Zip2 in 1995 and later founded SpaceX and X.com. He also founded X, the social media platform. Finally, Musk discussed his long career, his personal journey, and his achievements.\n",
            "\n",
            "Your summary should be a single, concise paragraph focusing on the key ideas and important points.\n",
            "\n",
            "Original Text:\n",
            "\n",
            "Musk is known for his roles in SpaceX, Tesla, and ownership of X. As a wealthy South African, he obtained citizenship through a Canadian-born mother. He attended Queen's University in Canada, studied physics and economics, and founded X in 2002. Musk is one of the wealthiest individuals and a leader in technology, engineering, and business. He started his career with X in 2002, leveraging earnings from X.com\n",
            "Summary: The following is a detailed description of Elon Musks life and his significant contributions to business, science, and technology.\n",
            "In conclusion, Musk is one of the wealthiest individuals in the world and a key leader in various fields, including technology, engineering, and business.\n",
            "He is a South African man with an Indian mother, who moved from Pretoria to Canada at 18. He attended Queens University in Canada, studied physics and economics, then worked in the automotive industry in Canada. He started a company called Zip2 in 1995 and later founded SpaceX and X.com. He also founded X, the social media platform. Finally, Musk discussed his long career, his personal journey, and his achievements.\n",
            "\n",
            "Your summary should be a single, concise paragraph focusing on the key ideas and important points.\n",
            "\n",
            "Original Text\n",
            "\n",
            "Musk is known for his roles in SpaceX, Tesla, and ownership of X. As a wealthy South African, he obtained citizenship through a Canadianborn mother. He attended Queens University in Canada, studied physics and economics, and founded X in 2002. Musk is one of the wealthiest individuals and a leader in technology, engineering, and business. He started his career with X in 2002, leveraging earnings from X.com\n"
          ]
        }
      ],
      "source": [
        "hf = HuggingFacePipeline(pipeline=_model)\n",
        "\n",
        "# Tạo prompt cho task tóm tắt văn bản\n",
        "template = f\"\"\"\n",
        "Summarize the following text into a single, concise paragraph focusing on the key ideas and important points:\n",
        "\n",
        "Text:\n",
        "{context}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "final_summary = \"\"\n",
        "try:\n",
        "\n",
        "\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "    chain = prompt | hf\n",
        "\n",
        "    result =  chain.invoke({\"context\": context})\n",
        "    print(result)\n",
        "\n",
        "    # Sử dụng regex để trích xuất phần Summary\n",
        "    summary = re.search(r\"Summary:\\s*(.+)\", result, re.DOTALL)\n",
        "\n",
        "    if summary:\n",
        "        final_summary = re.sub(r\"[^\\w\\s.,!?]\", \"\", summary.group(1)).strip()\n",
        "        print(\"Summary:\", final_summary)\n",
        "    else:\n",
        "        print(\"No summary found.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UKOrYCK09FLT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKOrYCK09FLT",
        "outputId": "131b4966-98a0-4fed-d1b7-c95d27108ebd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The text is: Elon Reeve Musk FRS (/ˈiːlɒn/; born June 28, 1971) is a businessman known for his key roles in the space company SpaceX and the automotive company Tesla, Inc. His other involvements include ownership of X Corp., the company that operates the social media platform X (formerly Twitter), and his role in the founding of the Boring Company, xAI, Neuralink, and OpenAI. Musk is the wealthiest individual in the world; as of December 2024, Forbes estimates his net worth to be US$432 billion.[2]. A member of the wealthy South African Musk family, Musk was born in Pretoria and briefly attended the University of Pretoria before immigrating to Canada at the age of 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University at Kingston in Canada. Musk later transferred to the University of Pennsylvania and received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University but never enrolled in classes, and with his brother Kimbal co-founded the online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999. That same year, Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal. In 2002, Musk acquired US citizenship, and that October eBay acquired PayPal for $1.5 billion. Using $100 million of the money he made from the sale of PayPal, Musk founded SpaceX, a spaceflight services company, in 2002.\n",
            "\n",
            "Extract all named entities from the context and classify them into the categories:\n",
            "['spacex', 'paypal']\n",
            "\n",
            "Named Entities-classification:\n",
            "Each named entity should be extracted once, and then assigned to one of the two categories:'spacex' or 'paypal'. Only categories that appear in the text should be considered.\n",
            "</think>\n",
            "\n",
            "**Extracted Named Entities:**\n",
            "- SpaceX\n",
            "- Tesla, Inc.\n",
            "- X Corp.\n",
            "- X (formerly Twitter)\n",
            "-openAI\n",
            "-Zip2\n",
            "-X.com\n",
            "-PayPal\n",
            "\n",
            "**Classification:**\n",
            "-'spacex' - SpaceX\n",
            "- 'paypal' - PayPal\n"
          ]
        }
      ],
      "source": [
        "hf = HuggingFacePipeline(pipeline=_model)\n",
        "\n",
        "# Tạo prompt cho task NER\n",
        "template = f\"\"\"\n",
        "The text is: {context}\n",
        "\n",
        "Extract all named entities from the context and classify them into the categories:\n",
        "{categories}\n",
        "\n",
        "Named Entities-classification:\n",
        "\"\"\"\n",
        "\n",
        "final_entities = \"\"\n",
        "try:\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "    chain = prompt | hf\n",
        "\n",
        "    result = chain.invoke({\"context\": context,\"categories\":categories})\n",
        "    print(result)\n",
        "\n",
        "    # Trích xuất phần \"Named Entities-classification:\" và parse các NER\n",
        "    ner_classification = re.search(r\"Named Entities-classification:\\s*(.*)\", result, re.DOTALL)\n",
        "\n",
        "    if ner_classification:\n",
        "        # Lấy danh sách các entity từ kết quả, chia theo dòng\n",
        "        final_entities = ner_classification.group(1).strip()\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EP2YN5wetRQG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP2YN5wetRQG",
        "outputId": "0b17bb1a-309c-45d1-eb0d-c8f629989c5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.14.0)\n",
            "Collecting langchain_huggingface\n",
            "  Using cached langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.8)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.7.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (14.2)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.3.32)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (3.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Using cached langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Downloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, httpx-sse, typing-inspect, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pydantic-settings, nvidia-cusolver-cu12, dataclasses-json, langchain_huggingface, langchain_community\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.16 langchain_huggingface-0.1.2 marshmallow-3.26.1 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio langchain_huggingface langchain langchain_community transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T4IfvH2qtED4",
      "metadata": {
        "id": "T4IfvH2qtED4"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "# from langchain.chains import LLMChain\n",
        "import re\n",
        "\n",
        "\n",
        "def qa_with_context(model, context, question):\n",
        "    # Create HuggingFacePipeline from the model\n",
        "    hf = HuggingFacePipeline(pipeline=model)\n",
        "    print(\"context:\", context)\n",
        "    print(\"question:\", question)\n",
        "    # Context text that the model will use to answer the question\n",
        "    # context = \"\"\"\n",
        "    # Albert Einstein was a theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics.\n",
        "    # \"\"\"\n",
        "\n",
        "    # Create a prompt template for QA with context\n",
        "    template = f\"\"\"\n",
        "    Here is the context:\n",
        "    {context}\n",
        "\n",
        "    Based on the above context, provide an answer to the following question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    final_answer = \"\"\n",
        "    try:\n",
        "        # Create a prompt from the template, using context and question\n",
        "        # qa_prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "        # # Create an LLMChain with Llama for contextual QA\n",
        "        # qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\n",
        "\n",
        "        # # Run the QA chain with context and question\n",
        "        # result = qa_chain.run({\"context\": context, \"question\": question})\n",
        "        # print(result)\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "        chain = prompt | hf\n",
        "        result = chain.invoke({\"question\": question, \"context\": context})\n",
        "\n",
        "        # Run the QA chain with context and question\n",
        "        # result = qa_chain.run({\"context\": context, \"question\": question})\n",
        "        # print(result)\n",
        "\n",
        "        answer = re.search(r\"Answer:\\s*(.*)\", result)\n",
        "        # Kiểm tra và lấy phần trả lời, loại bỏ khoảng trắng và dấu chấm\n",
        "        if answer:\n",
        "            final_answer = re.sub(r\"[^\\w\\s]\", \"\", answer.group(1)).strip() # Loại bỏ dấu chấm ở cuối nếu có\n",
        "            print(final_answer)\n",
        "        else:\n",
        "            print(\"No answer found.\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    return final_answer\n",
        "\n",
        "\n",
        "def qa_without_context(model, question):\n",
        "    # Create HuggingFacePipeline from the model\n",
        "    hf = HuggingFacePipeline(pipeline=model)\n",
        "    print(\"question\")\n",
        "    # Context text that the model will use to answer the question\n",
        "    # context = \"\"\"\n",
        "    # Albert Einstein was a theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics.\n",
        "    # \"\"\"\n",
        "\n",
        "    # Create a prompt template for QA with context\n",
        "    template = f\"\"\"\n",
        "    Please answer the following question to the best of your ability:\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    final_answer = \"\"\n",
        "    try:\n",
        "\n",
        "        # Tạo PromptTemplate chỉ với câu hỏi\n",
        "        # qa_no_context_prompt_template = PromptTemplate(template=qa_no_context_prompt, input_variables=[\"question\"])\n",
        "\n",
        "        # # Tạo LLMChain cho QA không cần ngữ cảnh\n",
        "        # qa_no_context_chain = LLMChain(llm=llm, prompt=qa_no_context_prompt_template)\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "        chain = prompt | hf\n",
        "        result = chain.invoke({\"question\": question})\n",
        "\n",
        "        # Chạy mô hình để trả lời câu hỏi\n",
        "        # result = qa_no_context_chain.run({\"question\": question})\n",
        "\n",
        "        answer = re.search(r\"Answer:\\s*(.*)\", result)\n",
        "        # Kiểm tra và lấy phần trả lời, loại bỏ khoảng trắng và dấu chấm\n",
        "        if answer:\n",
        "            final_answer = re.sub(r\"[^\\w\\s]\", \"\", answer.group(1)).strip() # Loại bỏ dấu chấm ở cuối nếu có\n",
        "            print(final_answer)\n",
        "        else:\n",
        "            print(\"No answer found.\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    return final_answer\n",
        "\n",
        "def text_classification(model, context, categories):\n",
        "    hf = HuggingFacePipeline(pipeline=model)\n",
        "\n",
        "\n",
        "    # Tạo prompt cho task phân loại văn bản\n",
        "    template = f\"\"\"Classify the following text into one of the following categories:\n",
        "    {categories}\n",
        "\n",
        "    The text is: {context}\n",
        "\n",
        "    Classification:\n",
        "    \"\"\"\n",
        "\n",
        "    final_answer = \"\"\n",
        "    try:\n",
        "       # Tạo prompt từ template, sử dụng văn bản cần phân loại\n",
        "        # classification_prompt_template = PromptTemplate(template=classification_prompt, input_variables=[\"context\"])\n",
        "\n",
        "        # # Tạo LLMChain cho task phân loại văn bản\n",
        "        # classification_chain = LLMChain(llm=llm, prompt=classification_prompt_template)\n",
        "\n",
        "        # # Chạy mô hình với văn bản cần phân loại\n",
        "        # result = classification_chain.run({\"context\": context})\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "        chain = prompt | hf\n",
        "        result = chain.invoke({\"context\": context,\"categories\":categories})\n",
        "\n",
        "        # Sử dụng regex để trích xuất phần Classification\n",
        "        classification = re.search(r\"Classification:\\s*(.*)\", result)\n",
        "\n",
        "        if classification:\n",
        "            final_answer = re.sub(r\"[^\\w\\s]\", \"\", classification.group(1)).strip()\n",
        "            print(\"Classification:\", final_answer)\n",
        "        else:\n",
        "            print(\"No classification found.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    return final_answer\n",
        "\n",
        "def text_summarization(model, context):\n",
        "    hf = HuggingFacePipeline(pipeline=model)\n",
        "\n",
        "    # Tạo prompt cho task tóm tắt văn bản\n",
        "    template = f\"\"\"\n",
        "    Summarize the following text into a single, concise paragraph focusing on the key ideas and important points:\n",
        "\n",
        "    Text:\n",
        "    {context}\n",
        "\n",
        "    Summary:\n",
        "    \"\"\"\n",
        "\n",
        "    final_summary = \"\"\n",
        "    try:\n",
        "        # Tạo prompt từ template, sử dụng văn bản cần tóm tắt\n",
        "        # summary_prompt_template = PromptTemplate(template=summary_prompt, input_variables=[\"context\"])\n",
        "\n",
        "        # # Tạo LLMChain cho task tóm tắt văn bản\n",
        "        # summary_chain = LLMChain(llm=llm, prompt=summary_prompt_template)\n",
        "\n",
        "        # # Chạy mô hình với văn bản cần tóm tắt\n",
        "        # result = summary_chain.run({\"context\": context})\n",
        "\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "        chain = prompt | hf\n",
        "\n",
        "        result =  chain.invoke({\"context\": context})\n",
        "        print(result)\n",
        "\n",
        "        # Sử dụng regex để trích xuất phần Summary\n",
        "        summary = re.search(r\"Summary:\\s*(.+)\", result, re.DOTALL)\n",
        "\n",
        "        if summary:\n",
        "            final_summary = re.sub(r\"[^\\w\\s.,!?]\", \"\", summary.group(1)).strip()\n",
        "            print(\"Summary:\", final_summary)\n",
        "        else:\n",
        "            print(\"No summary found.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "def text_ner(model, context, categories):\n",
        "    hf = HuggingFacePipeline(pipeline=model)\n",
        "\n",
        "    # Tạo prompt cho task NER\n",
        "    template = f\"\"\"\n",
        "    The text is: {context}\n",
        "\n",
        "    Extract all named entities from the context and classify them into the categories:\n",
        "    {categories}\n",
        "\n",
        "    Named Entities-classification:\n",
        "    \"\"\"\n",
        "\n",
        "    final_entities = \"\"\n",
        "    try:\n",
        "        # Tạo prompt từ template, sử dụng văn bản cần phân tích NER\n",
        "        # ner_prompt_template = PromptTemplate(template=ner_prompt, input_variables=[\"context\"])\n",
        "\n",
        "        # # Tạo LLMChain cho task NER\n",
        "        # ner_chain = LLMChain(llm=llm, prompt=ner_prompt_template)\n",
        "\n",
        "        # # Chạy mô hình với văn bản cần phân tích\n",
        "        # result = ner_chain.run({\"context\": context})\n",
        "\n",
        "        # print(\"Raw Result:\\n\", result)\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "        chain = prompt | hf\n",
        "\n",
        "        result = chain.invoke({\"context\": context,\"categories\":categories})\n",
        "        print(result)\n",
        "\n",
        "        # Trích xuất phần \"Named Entities-classification:\" và parse các NER\n",
        "        ner_classification = re.search(r\"Named Entities-classification:\\s*(.*)\", result, re.DOTALL)\n",
        "\n",
        "        if ner_classification:\n",
        "            # Lấy danh sách các entity từ kết quả, chia theo dòng\n",
        "            final_entities = ner_classification.group(1).strip()\n",
        "\n",
        "            # entities = entities_text.split(\"\\n\")\n",
        "\n",
        "            # # Duyệt qua các entity và chuyển đổi thành format mong muốn\n",
        "            # for entity in entities:\n",
        "            #     match = re.match(r\"\\d+\\.\\s*(\\w+):\\s*(.*)\", entity.strip())\n",
        "            #     if match:\n",
        "            #         entity_type = match.group(1).upper()  # Loại entity (Person, Location, Organization)\n",
        "            #         entity_value = match.group(2).strip()  # Giá trị entity\n",
        "\n",
        "            #         # Kiểm tra nếu value có nhiều địa điểm, tách ra\n",
        "            #         if entity_type == 'LOCATION' and ',' in entity_value:\n",
        "            #             # Tách value nếu chứa dấu phẩy\n",
        "            #             location_values = [val.strip() for val in entity_value.split(',')]\n",
        "            #             # Thêm từng phần vào final_entities dưới dạng các đối tượng riêng biệt\n",
        "            #             for location in location_values:\n",
        "            #                 final_entities.append({\"type\": \"LOCATION\", \"value\": location})\n",
        "            #         else:\n",
        "            #             # Thêm entity vào danh sách nếu không phải LOCATION hoặc không có dấu phẩy\n",
        "            #             final_entities.append({\"type\": entity_type, \"value\": entity_value})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    return final_entities\n",
        "\n",
        "def chatbot_with_history(model, conversation_history, user_input):\n",
        "    \"\"\"\n",
        "    Chatbot function that incorporates conversational history for more context-aware responses.\n",
        "\n",
        "    Args:\n",
        "        model: The language model for generating responses.\n",
        "        conversation_history: A list of tuples containing (user_message, bot_response) pairs.\n",
        "        user_input: The latest message from the user.\n",
        "\n",
        "    Returns:\n",
        "        bot_response: The chatbot's response to the user input.\n",
        "    \"\"\"\n",
        "    # Create HuggingFacePipeline from the model\n",
        "    hf = HuggingFacePipeline(pipeline=model)\n",
        "\n",
        "    # Format conversation history into a dialogue structure\n",
        "    # history_text = \"\\n\".join(\n",
        "    #     [f\"User: {user_msg}\\nBot: {bot_msg}\" for user_msg, bot_msg in conversation_history]\n",
        "    # )\n",
        "    # history_text = \"\\n\".join([f\"User: {m['text']}\" if m['is_user'] else f\"Bot: {m['text']}\" for m in conversation_history])\n",
        "    history_text = \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in conversation_history])\n",
        "\n",
        "    # Add the latest user input to the dialogue\n",
        "    template = f\"\"\"\n",
        "    You are a helpful and friendly chatbot. Below is the conversation history:\n",
        "\n",
        "    {history_text}\n",
        "\n",
        "    Now, the user says:\n",
        "    {user_input}\n",
        "\n",
        "    Respond to the user in a thoughtful and engaging manner:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Create a prompt from the template\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "        # Define the pipeline for processing the input through the model\n",
        "        chain = prompt | hf\n",
        "        result = chain.invoke({\"user_input\": user_input, \"conversation_history\": conversation_history}).strip()\n",
        "        print(\"result response:\", result, \"============\")\n",
        "\n",
        "        # Use regex to extract the response part\n",
        "        match = re.search(r\"Respond to the user in a thoughtful and engaging manner:\\s*(.*)\", result, re.DOTALL)\n",
        "        bot_response = match.group(1).splitlines()[0].strip() if match else \"Sorry, I couldn't understand the response.\"\n",
        "\n",
        "        if \"User:\" in bot_response:\n",
        "            match = re.search(r\"Assistant:\\s*\\\"?(.*?)(?=\\n|$)\", bot_response, re.DOTALL)\n",
        "            bot_response = match.group(1).strip()\n",
        "\n",
        "        print(\"chatbot response:\", bot_response, \"============\")\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "        bot_response = \"Sorry, I encountered an error. Could you rephrase your message?\"\n",
        "\n",
        "    # Update the conversation history\n",
        "    # conversation_history.append((user_input, bot_response))\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
        "\n",
        "    print(conversation_history)\n",
        "    return conversation_history, \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pd9_iuLarxqj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "Pd9_iuLarxqj",
        "outputId": "58fe3bcc-6656-4167-89cb-42eb34180226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is available.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on public URL: https://22a4f330d7efbebcf8.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://22a4f330d7efbebcf8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://22a4f330d7efbebcf8.gradio.live\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "task = \"text-generation\"\n",
        "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "project_id = 1\n",
        "\n",
        "from huggingface_hub import login\n",
        "hf_access_token = \"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\"\n",
        "login(token = hf_access_token)\n",
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.is_bf16_supported():\n",
        "        dtype = torch.bfloat16\n",
        "    else:\n",
        "        dtype = torch.float16\n",
        "\n",
        "    print(\"CUDA is available.\")\n",
        "\n",
        "    _model = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", #\"meta-llama/Llama-3.2-1B-Instruct\", #\"meta-llama/Llama-3.2-3B\", meta-llama/Llama-3.3-70B-Instruct\n",
        "        torch_dtype=dtype,\n",
        "        device_map=\"auto\",  # Hoặc có thể thử \"cpu\" nếu không ổn,\n",
        "        max_new_tokens=256,\n",
        "        token = \"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\"\n",
        "    )\n",
        "else:\n",
        "    print(\"No GPU available, using CPU.\")\n",
        "    _model = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", #\"meta-llama/Llama-3.2-1B-Instruct\", #\"meta-llama/Llama-3.2-3B\", meta-llama/Llama-3.3-70B-Instruct\n",
        "        device_map=\"cpu\",\n",
        "        max_new_tokens=256,\n",
        "        token = \"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\"\n",
        "    )\n",
        "def load_model(task,model_id, project_id, temperature=None, top_p=None, top_k=None, max_new_token=None):\n",
        "    from huggingface_hub import login\n",
        "    hf_access_token = \"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\"\n",
        "    login(token = hf_access_token)\n",
        "    if torch.cuda.is_available():\n",
        "        if torch.cuda.is_bf16_supported():\n",
        "            dtype = torch.bfloat16\n",
        "        else:\n",
        "            dtype = torch.float16\n",
        "\n",
        "        print(\"CUDA is available.\")\n",
        "\n",
        "        if not temperature:\n",
        "            _model = pipeline(\n",
        "            task,\n",
        "            model=model_id,\n",
        "            torch_dtype=dtype,\n",
        "            device_map=\"auto\",  # Hoặc có thể thử \"cpu\" nếu không ổn,\n",
        "            # max_new_tokens=256,\n",
        "            token = \"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\"\n",
        "            )\n",
        "        else:\n",
        "            _model = pipeline(\n",
        "                task,\n",
        "                model=model_id,\n",
        "                torch_dtype=dtype,\n",
        "                device_map=\"auto\",  # Hoặc có thể thử \"cpu\" nếu không ổn,\n",
        "                # max_new_tokens=256,\n",
        "                token = \"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\",\n",
        "                max_new_tokens=int(max_new_token),\n",
        "                temperature=float(temperature),\n",
        "                top_k=float(top_k),\n",
        "                top_p=float(top_p)\n",
        "            )\n",
        "    else:\n",
        "        print(\"No GPU available, using CPU.\")\n",
        "        if not temperature:\n",
        "            _model = pipeline(\n",
        "            task,\n",
        "            model=model_id,\n",
        "            torch_dtype=dtype,\n",
        "            device_map=\"auto\",  # Hoặc có thể thử \"cpu\" nếu không ổn,\n",
        "            # max_new_tokens=256,\n",
        "            token = \"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\"\n",
        "            )\n",
        "        else:\n",
        "            _model = pipeline(\n",
        "                task,\n",
        "                model=model_id,\n",
        "                device_map=\"cpu\",\n",
        "                # max_new_tokens=256,\n",
        "                token = \"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\",\n",
        "                max_new_tokens=int(max_new_token),\n",
        "                temperature=float(temperature),\n",
        "                top_k=float(top_k),\n",
        "                top_p=float(top_p),\n",
        "            )\n",
        "\n",
        "    return _model\n",
        "\n",
        "# load_model(task,model_id,project_id)\n",
        "\n",
        "def generate_response(input_text, temperature, top_p, top_k, max_new_token):\n",
        "    load_model(\"text-generation\", model_id, project_id, temperature, top_p, top_k, max_new_token)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\":  \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": input_text},\n",
        "    ]\n",
        "\n",
        "    result = _model(messages, max_length=1024)\n",
        "    generated_text = result[0]['generated_text']\n",
        "    print(result)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "def summarization_response(input_text, temperature, top_p, top_k, max_new_token):\n",
        "    load_model(\"text-generation\", model_id, project_id, temperature, top_p, top_k, max_new_token)\n",
        "    # _model = load_model(model_id)\n",
        "    generated_text = text_summarization(_model, input_text)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "def text_classification_response(input_text,categories_text, temperature, top_p, top_k, max_new_token):\n",
        "    load_model(\"text-generation\", model_id, project_id, temperature, top_p, top_k, max_new_token)\n",
        "    generated_text = text_classification(_model, input_text, categories_text)\n",
        "    return generated_text\n",
        "\n",
        "def question_answering_response(context_textbox,question_textbox, temperature, top_p, top_k, max_new_token):\n",
        "    load_model(\"text-generation\", model_id, project_id, temperature, top_p, top_k, max_new_token)\n",
        "    # _model = load_model(model_id)\n",
        "    if input_text and question_textbox:\n",
        "        generated_text = qa_with_context(_model, context_textbox, question_textbox)\n",
        "    elif context_textbox and not question_textbox:\n",
        "        generated_text = qa_without_context(_model, question_textbox)\n",
        "    else:\n",
        "        generated_text = qa_with_context(_model, question_textbox)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "def chatbot_continuous_chat(history, user_input, temperature, top_p, top_k, max_new_token):\n",
        "    load_model(\"text-generation\", model_id, project_id, temperature, top_p, top_k, max_new_token)\n",
        "    generated_text = chatbot_with_history(_model, history, user_input)\n",
        "    return generated_text\n",
        "\n",
        "with gr.Blocks() as demo_text_generation:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Group():\n",
        "                input_text = gr.Textbox(label=\"Input text\")\n",
        "                # prompt_text = gr.Textbox(label=\"Prompt text\")\n",
        "                temperature = gr.Slider(\n",
        "                    label=\"Temperature\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=100.0,\n",
        "                    step=0.1,\n",
        "                    value=0.9\n",
        "                )\n",
        "                top_p = gr.Slider(\n",
        "                    label=\"Top_p\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=1.0,\n",
        "                    step=0.1,\n",
        "                    value=0.6\n",
        "                )\n",
        "                top_k = gr.Slider(\n",
        "                    label=\"Top_k\",\n",
        "                    minimum=0,\n",
        "                    maximum=100,\n",
        "                    step=1,\n",
        "                    value=0\n",
        "                )\n",
        "                max_new_token = gr.Slider(\n",
        "                    label=\"Max new tokens\",\n",
        "                    minimum=1,\n",
        "                    maximum=1024,\n",
        "                    step=1,\n",
        "                    value=256\n",
        "                )\n",
        "            btn = gr.Button(\"Submit\")\n",
        "        with gr.Column():\n",
        "            output_text = gr.Textbox(label=\"Output text\")\n",
        "\n",
        "    btn.click(\n",
        "        fn=generate_response,\n",
        "        inputs=[input_text, temperature, top_p, top_k, max_new_token],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "with gr.Blocks() as demo_summarization:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Group():\n",
        "                input_text = gr.Textbox(label=\"Input text\")\n",
        "                temperature = gr.Slider(\n",
        "                    label=\"Temperature\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=100.0,\n",
        "                    step=0.1,\n",
        "                    value=0.9\n",
        "                )\n",
        "                top_p = gr.Slider(\n",
        "                    label=\"Top_p\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=1.0,\n",
        "                    step=0.1,\n",
        "                    value=0.6\n",
        "                )\n",
        "                top_k = gr.Slider(\n",
        "                    label=\"Top_k\",\n",
        "                    minimum=0,\n",
        "                    maximum=100,\n",
        "                    step=1,\n",
        "                    value=0\n",
        "                )\n",
        "                max_new_token = gr.Slider(\n",
        "                    label=\"Max new tokens\",\n",
        "                    minimum=1,\n",
        "                    maximum=1024,\n",
        "                    step=1,\n",
        "                    value=256\n",
        "                )\n",
        "                # prompt_text = gr.Textbox(label=\"Prompt text\")\n",
        "            btn = gr.Button(\"Submit\")\n",
        "        with gr.Column():\n",
        "            output_text = gr.Textbox(label=\"Output text\")\n",
        "    btn.click(\n",
        "        fn=summarization_response,\n",
        "        inputs=[input_text, temperature, top_p, top_k, max_new_token],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "with gr.Blocks() as demo_question_answering:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Group():\n",
        "                context_textbox = gr.Textbox(label=\"Context text\")\n",
        "                question_textbox = gr.Textbox(label=\"Question text\")\n",
        "                temperature = gr.Slider(\n",
        "                    label=\"Temperature\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=100.0,\n",
        "                    step=0.1,\n",
        "                    value=0.9\n",
        "                )\n",
        "                top_p = gr.Slider(\n",
        "                    label=\"Top_p\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=1.0,\n",
        "                    step=0.1,\n",
        "                    value=0.6\n",
        "                )\n",
        "                top_k = gr.Slider(\n",
        "                    label=\"Top_k\",\n",
        "                    minimum=0,\n",
        "                    maximum=100,\n",
        "                    step=1,\n",
        "                    value=0\n",
        "                )\n",
        "                max_new_token = gr.Slider(\n",
        "                    label=\"Max new tokens\",\n",
        "                    minimum=1,\n",
        "                    maximum=1024,\n",
        "                    step=1,\n",
        "                    value=256\n",
        "                )\n",
        "\n",
        "            btn = gr.Button(\"Submit\")\n",
        "        with gr.Column():\n",
        "            output_text =   gr.Textbox(label=\"Response:\")\n",
        "\n",
        "    btn.click(\n",
        "        fn=question_answering_response,\n",
        "        inputs=[context_textbox, question_textbox, temperature, top_p, top_k, max_new_token],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "with gr.Blocks() as demo_text_classification:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Group():\n",
        "                input_text = gr.Textbox(label=\"Input text\")\n",
        "                categories_text = gr.Textbox(label=\"Categories text\")\n",
        "                temperature = gr.Slider(\n",
        "                    label=\"Temperature\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=100.0,\n",
        "                    step=0.1,\n",
        "                    value=0.9\n",
        "                )\n",
        "                top_p = gr.Slider(\n",
        "                    label=\"Top_p\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=1.0,\n",
        "                    step=0.1,\n",
        "                    value=0.6\n",
        "                )\n",
        "                top_k = gr.Slider(\n",
        "                    label=\"Top_k\",\n",
        "                    minimum=0,\n",
        "                    maximum=100,\n",
        "                    step=1,\n",
        "                    value=0\n",
        "                )\n",
        "                max_new_token = gr.Slider(\n",
        "                    label=\"Max new tokens\",\n",
        "                    minimum=1,\n",
        "                    maximum=1024,\n",
        "                    step=1,\n",
        "                    value=256\n",
        "                )\n",
        "            btn = gr.Button(\"Submit\")\n",
        "        with gr.Column():\n",
        "            output_text = gr.Textbox(label=\"Response:\")\n",
        "\n",
        "    btn.click(\n",
        "        fn=text_classification_response,\n",
        "        inputs=[input_text, categories_text, temperature, top_p, top_k, max_new_token],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "def sentiment_classifier(text, temperature, top_p, top_k, max_new_token):\n",
        "    try:\n",
        "        sentiment_classifier = pipeline(\"sentiment-analysis\", temperature=temperature, top_p=top_p, top_k=top_k, max_new_token=max_new_token)\n",
        "        sentiment_response = sentiment_classifier(text)\n",
        "        # label = sentiment_response[0]['label']\n",
        "        # score = sentiment_response[0]['score']\n",
        "        print(sentiment_response)\n",
        "        import json\n",
        "        return json.dumps(sentiment_response)\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "with gr.Blocks() as demo_sentiment_analysis:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Group():\n",
        "                input_text = gr.Textbox(label=\"Input text\")\n",
        "                temperature = gr.Slider(\n",
        "                    label=\"Temperature\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=100.0,\n",
        "                    step=0.1,\n",
        "                    value=0.9\n",
        "                )\n",
        "                top_p = gr.Slider(\n",
        "                    label=\"Top_p\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=1.0,\n",
        "                    step=0.1,\n",
        "                    value=0.6\n",
        "                )\n",
        "                top_k = gr.Slider(\n",
        "                    label=\"Top_k\",\n",
        "                    minimum=0,\n",
        "                    maximum=100,\n",
        "                    step=1,\n",
        "                    value=0\n",
        "                )\n",
        "                max_new_token = gr.Slider(\n",
        "                    label=\"Max new tokens\",\n",
        "                    minimum=1,\n",
        "                    maximum=1024,\n",
        "                    step=1,\n",
        "                    value=256\n",
        "                )\n",
        "            btn = gr.Button(\"Submit\")\n",
        "        with gr.Column():\n",
        "\n",
        "            label_text = gr.Label(label=\"Label: \")\n",
        "            score_text = gr.Label(label=\"Score: \")\n",
        "    btn.click(\n",
        "        fn=sentiment_classifier,\n",
        "        inputs=[input_text, temperature, top_p, top_k, max_new_token],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "def predict_entities(input_text, categories_text, temperature, top_p, top_k, max_new_token):\n",
        "    load_model(\"text-generation\", model_id, project_id, temperature, top_p, top_k, max_new_token)\n",
        "    generated_text = text_ner(_model, input_text, categories_text)\n",
        "    return generated_text\n",
        "\n",
        "with gr.Blocks() as demo_ner:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Group():\n",
        "                input_text = gr.Textbox(label=\"Input text\")\n",
        "                categories_text = gr.Textbox(label=\"Categories text\")\n",
        "                temperature = gr.Slider(\n",
        "                    label=\"Temperature\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=100.0,\n",
        "                    step=0.1,\n",
        "                    value=0.9\n",
        "                )\n",
        "                top_p = gr.Slider(\n",
        "                    label=\"Top_p\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=1.0,\n",
        "                    step=0.1,\n",
        "                    value=0.6\n",
        "                )\n",
        "                top_k = gr.Slider(\n",
        "                    label=\"Top_k\",\n",
        "                    minimum=0,\n",
        "                    maximum=100,\n",
        "                    step=1,\n",
        "                    value=0\n",
        "                )\n",
        "                max_new_token = gr.Slider(\n",
        "                    label=\"Max new tokens\",\n",
        "                    minimum=1,\n",
        "                    maximum=1024,\n",
        "                    step=1,\n",
        "                    value=256\n",
        "                )\n",
        "            # with gr.Group():\n",
        "            #     input_text = gr.Textbox(label=\"Input text\")\n",
        "            btn = gr.Button(\"Submit\")\n",
        "        with gr.Column():\n",
        "            output_text = gr.Textbox(label=\"Response:\")\n",
        "\n",
        "    btn.click(\n",
        "        fn=predict_entities,\n",
        "        inputs=[input_text, categories_text, temperature, top_p, top_k, max_new_token],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "with gr.Blocks() as demo_text2text_generation:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Group():\n",
        "                input_text = gr.Textbox(label=\"Input text\", placeholder=\"Enter your text here\")\n",
        "                temperature = gr.Slider(\n",
        "                    label=\"Temperature\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=100.0,\n",
        "                    step=0.1,\n",
        "                    value=1\n",
        "                )\n",
        "                top_p = gr.Slider(\n",
        "                    label=\"Top_p\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=1.0,\n",
        "                    step=0.1,\n",
        "                    value=0.9\n",
        "                )\n",
        "                top_k = gr.Slider(\n",
        "                    label=\"Top_k\",\n",
        "                    minimum=0,\n",
        "                    maximum=100,\n",
        "                    step=1,\n",
        "                    value=0\n",
        "                )\n",
        "                max_new_token = gr.Slider(\n",
        "                    label=\"Max new tokens\",\n",
        "                    minimum=1,\n",
        "                    maximum=1024,\n",
        "                    step=1,\n",
        "                    value=256\n",
        "                )\n",
        "            btn = gr.Button(\"Submit\")\n",
        "        with gr.Column():\n",
        "            output_text = gr.Textbox(label=\"Output text\")\n",
        "\n",
        "    # Gắn sự kiện với nút Submit\n",
        "    btn.click(\n",
        "        fn=generate_response,\n",
        "        inputs=[input_text, temperature, top_p, top_k, max_new_token],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "with gr.Blocks() as demo_chatbot:\n",
        "    chatbot = gr.Chatbot(type=\"messages\")\n",
        "    with gr.Row():\n",
        "        user_input = gr.Textbox(label=\"Your message\", placeholder=\"Type your message here...\")\n",
        "    with gr.Row():\n",
        "        temperature = gr.Slider(\n",
        "                    label=\"Temperature\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=100.0,\n",
        "                    step=0.1,\n",
        "                    value=0.9\n",
        "                )\n",
        "        top_p = gr.Slider(\n",
        "            label=\"Top_p\",\n",
        "            minimum=0.0,\n",
        "            maximum=1.0,\n",
        "            step=0.1,\n",
        "            value=0.6\n",
        "        )\n",
        "        top_k = gr.Slider(\n",
        "            label=\"Top_k\",\n",
        "            minimum=0,\n",
        "            maximum=100,\n",
        "            step=1,\n",
        "            value=0\n",
        "        )\n",
        "        max_new_token = gr.Slider(\n",
        "            label=\"Max new tokens\",\n",
        "            minimum=1,\n",
        "            maximum=10000,\n",
        "            step=1,\n",
        "            value=1024\n",
        "        )\n",
        "    with gr.Row():\n",
        "        btn = gr.Button(\"Send\")\n",
        "\n",
        "    # Bind function to button click\n",
        "    btn.click(\n",
        "        fn=chatbot_continuous_chat,\n",
        "        inputs=[chatbot, user_input, temperature, top_p, top_k, max_new_token],  # Thêm các tham số vào hàm\n",
        "        outputs=[chatbot, user_input]  # Cập nhật lại chatbot với phản hồi mới và xóa trường nhập liệu\n",
        "    )\n",
        "\n",
        "\n",
        "DESCRIPTION = \"\"\"\\\n",
        "# LLM UI\n",
        "This is a demo of LLM UI.\n",
        "\"\"\"\n",
        "with gr.Blocks(css=\"style.css\") as demo:\n",
        "    gr.Markdown(DESCRIPTION)\n",
        "\n",
        "    with gr.Tabs():\n",
        "        if task == \"text-generation\":\n",
        "            with gr.Tab(label=task):\n",
        "                demo_text_generation.render()\n",
        "        elif task == \"summarization\":\n",
        "            with gr.Tab(label=task):\n",
        "                demo_summarization.render()\n",
        "        elif task == \"question-answering\":\n",
        "            with gr.Tab(label=task):\n",
        "                demo_question_answering.render()\n",
        "        elif task == \"text-classification\":\n",
        "            with gr.Tab(label=task):\n",
        "                    demo_text_classification.render()\n",
        "        elif task == \"sentiment-analysis\":\n",
        "            with gr.Tab(label=task):\n",
        "                demo_sentiment_analysis.render()\n",
        "        elif task == \"ner\":\n",
        "            with gr.Tab(label=task):\n",
        "                demo_ner.render()\n",
        "        # elif task == \"fill-mask\":\n",
        "        #   with gr.Tab(label=task):\n",
        "        #         demo_fill_mask.render()\n",
        "        elif task == \"text2text-generation\":\n",
        "            with gr.Tab(label=task):\n",
        "                demo_text2text_generation.render()\n",
        "        elif task == \"chat-bot\":\n",
        "            with gr.Tab(label=task):\n",
        "                demo_chatbot.render()\n",
        "\n",
        "\n",
        "gradio_app, local_url, share_url = demo.launch(share=True, quiet=True, prevent_thread_lock=True, server_name='0.0.0.0',show_error=True)\n",
        "print(share_url)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "017a7d602ad74796b8b1994081464c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d77dd181c3ba42fcba2205e00d6635d8",
            "max": 9846,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_754f9d8259c14c7ba9313c05d9e1dd7a",
            "value": 9846
          }
        },
        "02b927db24e5499e9b6d624ab9a1b006": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "063d4e56c5934065a6f8f0e70646cc67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aae5612884546aaa78ed8a37b3ef6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c6f1fddbf3f4fa5aa6747f14bcc5ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c73bdc9f1c2421a864120a7f30ebae6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "103e368a7abc4029a64511e9ec3c41e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_063d4e56c5934065a6f8f0e70646cc67",
            "max": 518,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d71e06570c245bda6b820a3db8ca110",
            "value": 518
          }
        },
        "12d1846dd4bd4bc083bd453a7ae2636e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16eb846b69e847a9875f3840f3352ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfe25ca9704a409d81cd731d44aa7d4b",
            "placeholder": "​",
            "style": "IPY_MODEL_5e47ac4a84634765b0bb8fd8d257e412",
            "value": "README.md: 100%"
          }
        },
        "1bd1f67ad14a474fb81083a3ebc2951f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "205bf599c4c641d0913dd75d8d7541c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26a3a6a9a2d44870bad378aa6d223c15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d2e0cd9caf4dad90cede23f5436848": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d71e06570c245bda6b820a3db8ca110": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e3b7557c586464c85133495aeec153c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3611ec7a1a7c425581f06673976be0ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c2655fc08e54138915588de0af525d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_438353e0676c4b498b5e60520634bd38",
            "placeholder": "​",
            "style": "IPY_MODEL_f3b75fda034c43669e4f5d103cf88810",
            "value": " 518/518 [00:01&lt;00:00, 326.33 examples/s]"
          }
        },
        "417c46bf99e84c12bc6959581b8ca508": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02b927db24e5499e9b6d624ab9a1b006",
            "placeholder": "​",
            "style": "IPY_MODEL_29d2e0cd9caf4dad90cede23f5436848",
            "value": " 9846/9846 [00:00&lt;00:00, 25174.71 examples/s]"
          }
        },
        "4192ea964dc24c9498f138d4c9af61e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "425e183e7e8e40a8b16c959e458031bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "438353e0676c4b498b5e60520634bd38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "464be5fb49db4bb39fb4c910ea2bcdcc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "489efbd6329b4150ae12e5644322ec5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ad2828781ea42499d337fc80abd0a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bd1f67ad14a474fb81083a3ebc2951f",
            "max": 20877686,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da90355165704ecab629de2581763aad",
            "value": 20877686
          }
        },
        "4b5901dd21884462bee8e251c6330c94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fdd821c9c9240418365250b8240d12d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50fb7dacbfe74b5eb7071b14e5bb42ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3611ec7a1a7c425581f06673976be0ef",
            "placeholder": "​",
            "style": "IPY_MODEL_0aae5612884546aaa78ed8a37b3ef6a9",
            "value": "openassistant_best_replies_train.jsonl: 100%"
          }
        },
        "56493cf9b2384e8e88d4e12279b62d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c0f9fc62597497fb319dae7f39d7273": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e47ac4a84634765b0bb8fd8d257e412": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e8619d5ed8a48af943a94fea5fb3722": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ebac1d5989f4e8daed196723f0be7c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f29f854e9074a4ba923336d86de356d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60f4b758cfa04e4a9895c06118e16f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b4dd46e060146c4b30a4b6582ad67d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f98a5531d8f412dbd55340f9d5aa6cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4418b69d61749fa8bf48d5dfc8abc79",
            "placeholder": "​",
            "style": "IPY_MODEL_7b2b65ef90e24672b0f6b1b4215c6699",
            "value": "Generating test split: 100%"
          }
        },
        "7022e1fe0b8b41deb432dbc80029ce4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74469960e4624e0faa09b8096e587e42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "754f9d8259c14c7ba9313c05d9e1dd7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7554b55cbffa4487b87e7e6f019783d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c0f9fc62597497fb319dae7f39d7273",
            "placeholder": "​",
            "style": "IPY_MODEL_60f4b758cfa04e4a9895c06118e16f2d",
            "value": " 1.11M/1.11M [00:00&lt;00:00, 16.7MB/s]"
          }
        },
        "7b2b65ef90e24672b0f6b1b4215c6699": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8359f8a9b13142a297ad615703f19c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86103c47134c4f4a81c347e8554379b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_205bf599c4c641d0913dd75d8d7541c7",
            "max": 1105272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_425e183e7e8e40a8b16c959e458031bf",
            "value": 1105272
          }
        },
        "8a9f401200e947e98f73f37685c82215": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f1eae3f8f1740f28798f69f5e408838": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf031fe019534a13b4ea9aada4782661",
              "IPY_MODEL_db1ceec0ead94d2b9bb5b727c98336fa",
              "IPY_MODEL_417c46bf99e84c12bc6959581b8ca508"
            ],
            "layout": "IPY_MODEL_5ebac1d5989f4e8daed196723f0be7c0"
          }
        },
        "8f5c1995063f4c89becece4e13e6fbfd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "974fbb0e7d544f44a4beeb1bf4f69c85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a079494321d2453e8886ba032f47591d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2c0faf25820445ba8186b06566c5fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a45ae5465342451dacde365096172a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ada6b90e1bd44d3ca647ca0da1c37caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e744323c0636464f980b6ea1cc25033f",
            "placeholder": "​",
            "style": "IPY_MODEL_4fdd821c9c9240418365250b8240d12d",
            "value": " 9846/9846 [00:29&lt;00:00, 379.52 examples/s]"
          }
        },
        "b135a5449b25433a8b91a198b2ded4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f98a5531d8f412dbd55340f9d5aa6cc",
              "IPY_MODEL_103e368a7abc4029a64511e9ec3c41e9",
              "IPY_MODEL_e30f25f1991846bbb9db91dc60903f9c"
            ],
            "layout": "IPY_MODEL_6b4dd46e060146c4b30a4b6582ad67d3"
          }
        },
        "b566bef94c5e48468e58d1b42fa5e3b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c6f1fddbf3f4fa5aa6747f14bcc5ca2",
            "max": 518,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12d1846dd4bd4bc083bd453a7ae2636e",
            "value": 518
          }
        },
        "b5bae5dc908f447fb01da787894c6fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf8add6bdaf046728defa40530e65e7d",
              "IPY_MODEL_017a7d602ad74796b8b1994081464c2f",
              "IPY_MODEL_ada6b90e1bd44d3ca647ca0da1c37caf"
            ],
            "layout": "IPY_MODEL_a079494321d2453e8886ba032f47591d"
          }
        },
        "bd8735715a054c83a69dea6e42fbd6dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b5901dd21884462bee8e251c6330c94",
            "placeholder": "​",
            "style": "IPY_MODEL_5f29f854e9074a4ba923336d86de356d",
            "value": " 395/395 [00:00&lt;00:00, 19.7kB/s]"
          }
        },
        "be997add2bd74bb988b14562c6fec61f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16eb846b69e847a9875f3840f3352ccc",
              "IPY_MODEL_c56d4d4a85414516b8264070461bc195",
              "IPY_MODEL_bd8735715a054c83a69dea6e42fbd6dd"
            ],
            "layout": "IPY_MODEL_974fbb0e7d544f44a4beeb1bf4f69c85"
          }
        },
        "c56d4d4a85414516b8264070461bc195": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c73bdc9f1c2421a864120a7f30ebae6",
            "max": 395,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a45ae5465342451dacde365096172a32",
            "value": 395
          }
        },
        "ced717d2ff5a46aea97f413fc04b3f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf031fe019534a13b4ea9aada4782661": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74469960e4624e0faa09b8096e587e42",
            "placeholder": "​",
            "style": "IPY_MODEL_4192ea964dc24c9498f138d4c9af61e6",
            "value": "Generating train split: 100%"
          }
        },
        "cf8add6bdaf046728defa40530e65e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7022e1fe0b8b41deb432dbc80029ce4d",
            "placeholder": "​",
            "style": "IPY_MODEL_8359f8a9b13142a297ad615703f19c42",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "cfe25ca9704a409d81cd731d44aa7d4b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4418b69d61749fa8bf48d5dfc8abc79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d77dd181c3ba42fcba2205e00d6635d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da90355165704ecab629de2581763aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db1ceec0ead94d2b9bb5b727c98336fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a9f401200e947e98f73f37685c82215",
            "max": 9846,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2c0faf25820445ba8186b06566c5fa9",
            "value": 9846
          }
        },
        "df1fbb0e29154c0a8e039265cf6a081a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f5c1995063f4c89becece4e13e6fbfd",
            "placeholder": "​",
            "style": "IPY_MODEL_5e8619d5ed8a48af943a94fea5fb3722",
            "value": " 20.9M/20.9M [00:00&lt;00:00, 90.7MB/s]"
          }
        },
        "dfeb9cbd42d0416e81905f266047865a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4c0747fe832434a95e6e55d385bd8f7",
              "IPY_MODEL_86103c47134c4f4a81c347e8554379b0",
              "IPY_MODEL_7554b55cbffa4487b87e7e6f019783d9"
            ],
            "layout": "IPY_MODEL_489efbd6329b4150ae12e5644322ec5d"
          }
        },
        "e30f25f1991846bbb9db91dc60903f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_464be5fb49db4bb39fb4c910ea2bcdcc",
            "placeholder": "​",
            "style": "IPY_MODEL_56493cf9b2384e8e88d4e12279b62d3d",
            "value": " 518/518 [00:00&lt;00:00, 5692.32 examples/s]"
          }
        },
        "e4c0747fe832434a95e6e55d385bd8f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7501f2a3d3c4cfa9deecdd9630cb09b",
            "placeholder": "​",
            "style": "IPY_MODEL_2e3b7557c586464c85133495aeec153c",
            "value": "openassistant_best_replies_eval.jsonl: 100%"
          }
        },
        "e6f6e7985ba345b7a839fcc3a4dad682": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f42885b072a64782ba49ab75b27eb30c",
            "placeholder": "​",
            "style": "IPY_MODEL_ced717d2ff5a46aea97f413fc04b3f66",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "e744323c0636464f980b6ea1cc25033f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7501f2a3d3c4cfa9deecdd9630cb09b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecf18e4b27a54504a7fd718eae009869": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6f6e7985ba345b7a839fcc3a4dad682",
              "IPY_MODEL_b566bef94c5e48468e58d1b42fa5e3b8",
              "IPY_MODEL_3c2655fc08e54138915588de0af525d3"
            ],
            "layout": "IPY_MODEL_26a3a6a9a2d44870bad378aa6d223c15"
          }
        },
        "f22981a360354ad39cb145fb35e0fbb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3b75fda034c43669e4f5d103cf88810": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f42885b072a64782ba49ab75b27eb30c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7a9e4aac7ea4a299556171d9ea1f169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50fb7dacbfe74b5eb7071b14e5bb42ad",
              "IPY_MODEL_4ad2828781ea42499d337fc80abd0a39",
              "IPY_MODEL_df1fbb0e29154c0a8e039265cf6a081a"
            ],
            "layout": "IPY_MODEL_f22981a360354ad39cb145fb35e0fbb8"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
